{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3db171ee",
   "metadata": {},
   "source": [
    "## Evaluación \n",
    "\n",
    "En este proyecto consideramos tres dimensiones clave para seleccionar el modelo más adecuado: tiempo de procesamiento, costos y precisión.\n",
    "\n",
    "**Tiempo de procesamiento**\n",
    "El tiempo que cada modelo tardó en procesar la carpeta de Cartas Documento fue muy similar entre las tres alternativas. Esta homogeneidad hace que, para este caso, el tiempo no sea un criterio relevante de decisión.\n",
    "\n",
    "**Costos**\n",
    "Tesseract OCR y Doctr se ejecutaron en la computadora local sin requerir recursos adicionales ni gastos en infraestructura o licencias.\n",
    "\n",
    "API de GPT implicó un costo variable asociado al consumo. Cada iteración completa sobre la carpeta de Cartas Documento (conversión imagen → texto) tuvo un costo aproximado de 0,66 USD.\n",
    "\n",
    "**Precisión**\n",
    "Esta es la métrica más compleja de estimar, ya que requiere un dataset de referencia (ground truth) para comparar las transcripciones.\n",
    "\n",
    "Como paso previo, generaremos un dataset target con transcripciones verificadas manualmente.\n",
    "\n",
    "Una vez disponible, calcularemos la distancia de Levenshtein entre cada transcripción generada por los modelos y el texto objetivo, obteniendo así una medida cuantitativa y comparable de la precisión de cada enfoque.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c6a08f",
   "metadata": {},
   "source": [
    "#### Construcción del target\n",
    " Antes de abordar el problema de la evaluación de los outputs, vamos a extraer entidades de los achivos transcriptos a mano. De esa manera, construimos nuestro target para poder comparar. Utilizamos la misma lógica de extracción de entidades que en los otros casos y usamos el modelo Llama 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34bf8663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "from pydantic import ValidationError\n",
    "import re, unicodedata\n",
    "from tqdm import tqdm\n",
    "from requests.exceptions import ReadTimeout\n",
    "from pathlib import Path\n",
    "import json\n",
    "import csv\n",
    "import requests\n",
    "from requests.exceptions import ReadTimeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00f9d47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TXT_FOLDER = Path(\"Transcripciones_manual\")\n",
    "OUTPUT_CSV = Path(\"entidades_extraidas_mauscritas.csv\")\n",
    "PROMPT_FILE = \"prompt_2.txt\"\n",
    "MODEL       = \"llama3:latest\"\n",
    "OLLAMA_URL  = \"http://localhost:11434/api/generate\"\n",
    "TEMPERATURE = 0.0\n",
    "TIMEOUT     = 900\n",
    "MAX_CHARS   = 9000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b4eeb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entity(BaseModel):\n",
    "    Remitente: Optional[str] = None\n",
    "    DNI: Optional[str] = None\n",
    "    CUIT_CUIL: Optional[str] = None\n",
    "    Cuerpo: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9efc6762",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_RE = re.compile(r\"[^\\w\\s.,;:()@€$%/-]\")\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "    s = CLEAN_RE.sub(\"\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29d9e1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"Remitente\": {\"type\": [\"string\", \"null\"]},\n",
    "        \"DNI\": {\"type\": [\"string\", \"null\"], \"pattern\": \"^\\\\d*$\"},\n",
    "        \"CUIT_CUIL\": {\"type\": [\"string\", \"null\"], \"pattern\": \"^\\\\d*$\"},\n",
    "        \"Cuerpo\": {\"type\": \"string\"},\n",
    "    },\n",
    "    \"required\": [\"Cuerpo\"],\n",
    "    \"additionalProperties\": False,\n",
    "}\n",
    "\n",
    "with open(PROMPT_FILE, encoding=\"utf-8\") as f:\n",
    "    PROMPT = f.read()\n",
    "\n",
    "def call_ollama(text: str) -> dict:\n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"prompt\": f\"<|system|>\\n{PROMPT}<|end|>\\n<|user|>\\n{text[:MAX_CHARS]}<|end|>\\n<|assistant|>\",\n",
    "        \"format\": \"json\",\n",
    "        \"stream\": False,\n",
    "        \"options\": {\"temperature\": TEMPERATURE, \"json_schema\": JSON_SCHEMA},\n",
    "    }\n",
    "    try:\n",
    "        r = requests.post(OLLAMA_URL, json=payload, timeout=TIMEOUT)\n",
    "    except ReadTimeout:\n",
    "        raise TimeoutError(\"Timeout de Ollama\")\n",
    "    r.raise_for_status()\n",
    "    return json.loads(r.json()[\"response\"])\n",
    "\n",
    "def main():\n",
    "    dir_path = Path(TXT_FOLDER)\n",
    "    if not dir_path.is_dir():\n",
    "        raise SystemExit(f\"Ruta no encontrada: {dir_path}\")\n",
    "\n",
    "    rows = []\n",
    "    for file in tqdm(sorted(dir_path.glob(\"*.txt\")), desc=\"TXT\"):\n",
    "        raw = file.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        cleaned = clean_text(raw)\n",
    "        try:\n",
    "            data = call_ollama(cleaned)\n",
    "            ent = Entity.model_validate(data)\n",
    "            rows.append({\n",
    "                \"ARCHIVO\": file.name,\n",
    "                \"Remitente\": (ent.Remitente or \"NaN\").strip() or \"NaN\",\n",
    "                \"DNI\": re.sub(r\"\\D\", \"\", ent.DNI or \"\") or \"NaN\",\n",
    "                \"CUIT_CUIL\": re.sub(r\"\\D\", \"\", ent.CUIT_CUIL or \"\") or \"NaN\",\n",
    "                \"Cuerpo\": ent.Cuerpo.strip() or \"NaN\",\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {file.name}: {e}\")\n",
    "    if rows:\n",
    "        with open(OUTPUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            csv.DictWriter(f, fieldnames=rows[0].keys()).writeheader(); csv.DictWriter(f, fieldnames=rows[0].keys()).writerows(rows)\n",
    "        print(f\"✅ CSV generado en {OUTPUT_CSV} ({len(rows)} filas)\")\n",
    "    else:\n",
    "        print(\"No se extrajeron entidades válidas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d0b02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TXT:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efefc83a",
   "metadata": {},
   "source": [
    "### Distancia de Levenshtein\n",
    "\n",
    "Una vez realizada la extracción de entidades con cada método, evaluaremos cuál obtuvo mejores resultados utilizando la **distancia de Levenshtein**.  \n",
    "\n",
    "Esta métrica mide el número mínimo de operaciones necesarias para transformar una cadena en otra. Las operaciones permitidas son:  \n",
    "- **Inserción** de un carácter.  \n",
    "- **Eliminación** de un carácter.  \n",
    "- **Sustitución** de un carácter por otro.  \n",
    "\n",
    "Un valor **0** indica coincidencia exacta. Cuanto mayor sea el valor, mayor es la diferencia entre las cadenas.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beabc39d",
   "metadata": {},
   "source": [
    "1. Instalamos librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085a8466",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91ebb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install jellyfish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31957722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import sys\n",
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import jellyfish\n",
    "except ModuleNotFoundError:\n",
    "    raise SystemExit(\n",
    "        f\"[ERROR] jellyfish no está instalado en este intérprete:\\n  {sys.executable}\\n\"\n",
    "        f\"Instalá con:\\n  \\\"{sys.executable}\\\" -m pip install jellyfish\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04412bc5",
   "metadata": {},
   "source": [
    "2. Definimos el path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3d65eb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path.cwd()\n",
    "\n",
    "CSV_TARGET_MANUS = BASE_DIR / \"entidades_extraidas_manuscritas.csv\"\n",
    "CSV_BASES: Dict[str, Path] = {\n",
    "    \"DOCTR\":     BASE_DIR / \"entidades_extraidas_DOCTR.csv\",\n",
    "    \"GPT-5\":     BASE_DIR / \"entidades_extraidas_GPT-5.csv\",\n",
    "    \"Tesseract\": BASE_DIR / \"entidades_extraidas_Tesseract.csv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754a9485",
   "metadata": {},
   "source": [
    "3. Definimos funciones de normalización para cada variable, para hacerlas más comparables entre sí.\n",
    "\n",
    "`quitar_acentos(text: str) -> str`\n",
    "Elimina acentos de las letras y preservamos la letra Ñ\n",
    "\n",
    "\n",
    "`MARCADORES_CUERPO`\n",
    "Lista de patrones regex que representan marcadores específicos a eliminar del cuerpo del texto. Esta función elimina cualquier caracter que no sea: letras, números, espacios, comas y puntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "28dfe45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quitar_acentos(text: str) -> str:\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    text = str(text).replace(\"ñ\", \"__enie__\").replace(\"Ñ\", \"__ENIE__\")\n",
    "    sin_acento = \"\".join(\n",
    "        c for c in unicodedata.normalize(\"NFD\", text)\n",
    "        if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "    return sin_acento.replace(\"__enie__\", \"ñ\").replace(\"__ENIE__\", \"Ñ\")\n",
    "\n",
    "\n",
    "MARCADORES_CUERPO = [r\"\\(\\?\\)\"]  \n",
    "\n",
    "def norm_cuerpo(x: str) -> str:\n",
    "    if not x:\n",
    "        return \"\"\n",
    "    t = quitar_acentos(x).lower()\n",
    "    for pat in MARCADORES_CUERPO:\n",
    "        t = re.sub(rf\"\\s*{pat}\\s*\", \" \", t)\n",
    "    # limpieza general: letras (incluyendo ñ), números, espacios, comas y puntos\n",
    "    t = re.sub(r\"[^a-z0-9ñÑ\\s.,]\", \"\", t)\n",
    "    # colapsar espacios\n",
    "    return \" \".join(t.split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a035f6",
   "metadata": {},
   "source": [
    "4. Funciones de lectura: Estas dos funciones homogeneizan los CSV de entrada y construyen una clave de emparejamiento por nombre de archivo. Esto permite que todos los datasets compartan el mismo esquema de columnas y una clave (archivo_norm) comparable entre sí.\n",
    "\n",
    "`estandarizar_cols` limpia y pone en formato estándar el DataFrame (nombres de columnas, tipos, clave archivo_norm).\n",
    "\n",
    "`cargar_csv` asegura que el archivo exista, lo lee como texto, lo estandariza y valida que tenga todo lo necesario para las comparaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d3053e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estandarizar_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.rename(columns={c: (c.strip() if isinstance(c, str) else c) for c in df.columns})\n",
    "    lower = {str(c).lower(): c for c in df.columns}\n",
    "\n",
    "    if 'archivo' in lower:\n",
    "        df = df.rename(columns={lower['archivo']: 'archivo'})\n",
    "    elif 'ARCHIVO' in df.columns:\n",
    "        df = df.rename(columns={'ARCHIVO': 'archivo'})\n",
    "\n",
    "    if 'remitente' in lower and 'Remitente' not in df.columns:\n",
    "        df = df.rename(columns={lower['remitente']: 'Remitente'})\n",
    "    if 'cuerpo' in lower and 'Cuerpo' not in df.columns:\n",
    "        df = df.rename(columns={lower['cuerpo']: 'Cuerpo'})\n",
    "    if 'dni' in lower and 'DNI' not in df.columns:\n",
    "        df = df.rename(columns={lower['dni']: 'DNI'})\n",
    "    for k in ['cuit_cuil','cuitcuil','cuit','cuil']:\n",
    "        if k in lower and 'CUIT_CUIL' not in df.columns:\n",
    "            df = df.rename(columns={lower[k]: 'CUIT_CUIL'})\n",
    "            break\n",
    "    if 'CUIT_CUIL' not in df.columns and 'Cuit_Cuil' in df.columns:\n",
    "        df = df.rename(columns={'Cuit_Cuil': 'CUIT_CUIL'})\n",
    "\n",
    "    # tipado y limpieza básica\n",
    "    for c in ['archivo','Remitente','DNI','CUIT_CUIL','Cuerpo']:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype('string')\n",
    "\n",
    "    # filtrar filas sin archivo y generar clave normalizada\n",
    "    if 'archivo' in df.columns:\n",
    "        df = df[df['archivo'].notna() & (df['archivo'].str.strip()!='')].copy()\n",
    "        df['archivo_norm'] = df['archivo'].map(norm_fname)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def cargar_csv(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"No existe el archivo: {path}\")\n",
    "    df = pd.read_csv(path, encoding='utf-8-sig', dtype=str)\n",
    "    df = estandarizar_cols(df)\n",
    "    req = {'archivo','Remitente','DNI','CUIT_CUIL','Cuerpo','archivo_norm'}\n",
    "    faltan = req - set(df.columns)\n",
    "    if faltan:\n",
    "        raise ValueError(f\"Faltan columnas en {path.name}: {faltan}. Encontradas: {list(df.columns)}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde5fc75",
   "metadata": {},
   "source": [
    "5.  Este bloque hace la comparación de las 7 cartas manuscritas contra cada base OCR (DOCTR, GPT‑5, Tesseract), calculando distancias de Levenshtein por campo. \n",
    "\n",
    "`comparar_base_con_target` Compara el target manuscrito (lado izquierdo) contra la base OCR (lado derecho), manteniendo SIEMPRE todas las filas del target. Luego calcula las distancias de similitud campo a campo.\n",
    "\n",
    "`comparar_bases_vs_target` Orquesta la comparación del target manuscrito contra todas las bases OCR disponibles. Para ello, aplica la normalización (norm_nombre, norm_cuerpo, norm_num) en los campos clave, llama a comparar_base_con_target para obtener distancias y consolida los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e5972da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparar_base_con_target(df_target: pd.DataFrame, df_base: pd.DataFrame, nombre_base: str) -> pd.DataFrame:\n",
    "        # Renombrar columnas para claridad\n",
    "    t = df_target[['archivo','Remitente','DNI','CUIT_CUIL','Cuerpo','archivo_norm']].copy().rename(\n",
    "        columns={'archivo':'archivo_target','Remitente':'Remitente_target','DNI':'DNI_target','CUIT_CUIL':'CUIT_CUIL_target','Cuerpo':'Cuerpo_target'}\n",
    "    )\n",
    "    b = df_base[['archivo','Remitente','DNI','CUIT_CUIL','Cuerpo','archivo_norm']].copy().rename(\n",
    "        columns={'archivo':'archivo_base','Remitente':'Remitente_base','DNI':'DNI_base','CUIT_CUIL':'CUIT_CUIL_base','Cuerpo':'Cuerpo_base'}\n",
    "    )\n",
    "\n",
    "    # Merge con left join desde el target \n",
    "    m = t.merge(b[['archivo_norm','archivo_base','Remitente_base','DNI_base','CUIT_CUIL_base','Cuerpo_base']],\n",
    "                on='archivo_norm', how='left')\n",
    "\n",
    "    dist_rem, dist_cue, dist_cue_norm, dist_dni, dist_cuit = [], [], [], [], []\n",
    "    for _, r in m.iterrows():\n",
    "        # Remitente\n",
    "        rt = norm_nombre(r.get('Remitente_target'))\n",
    "        rb = norm_nombre(r.get('Remitente_base')) if pd.notna(r.get('Remitente_base')) else None\n",
    "        drem = lev(rt, rb); dist_rem.append(pd.NA if drem is None else drem)\n",
    "\n",
    "        # Cuerpo\n",
    "        ct = norm_cuerpo(r.get('Cuerpo_target'))\n",
    "        cb = norm_cuerpo(r.get('Cuerpo_base')) if pd.notna(r.get('Cuerpo_base')) else None\n",
    "        dcue = lev(ct, cb); dist_cue.append(pd.NA if dcue is None else dcue)\n",
    "        if cb is None:\n",
    "            dist_cue_norm.append(pd.NA)\n",
    "        else:\n",
    "            mlen = max(len(ct or \"\"), len(cb or \"\")); dist_cue_norm.append(pd.NA if mlen==0 else dcue/mlen)\n",
    "\n",
    "        # DNI\n",
    "        dt = norm_num(r.get('DNI_target'))\n",
    "        db = norm_num(r.get('DNI_base')) if pd.notna(r.get('DNI_base')) else None\n",
    "        ddni = lev(dt, db); dist_dni.append(pd.NA if ddni is None else ddni)\n",
    "\n",
    "        # CUIT\n",
    "        qt = norm_num(r.get('CUIT_CUIL_target'))\n",
    "        qb = norm_num(r.get('CUIT_CUIL_base')) if pd.notna(r.get('CUIT_CUIL_base')) else None\n",
    "        dcui = lev(qt, qb); dist_cuit.append(pd.NA if dcui is None else dcui)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        'ARCHIVO_TARGET': m['archivo_target'],\n",
    "        'ARCHIVO_BASE': m['archivo_base'],\n",
    "        'archivo_norm': m['archivo_norm'],\n",
    "        'base': nombre_base,\n",
    "        'dist_remitente': pd.Series(dist_rem, dtype='Int64'),\n",
    "        'dist_cuerpo': pd.Series(dist_cue, dtype='Int64'),\n",
    "        'dist_cuerpo_norm': pd.Series(dist_cue_norm, dtype='Float64'),\n",
    "        'dist_dni': pd.Series(dist_dni, dtype='Int64'),\n",
    "        'dist_cuit': pd.Series(dist_cuit, dtype='Int64'),\n",
    "    })\n",
    "    return out\n",
    "\n",
    "\n",
    "def comparar_bases_vs_target(csv_target: Path = CSV_TARGET_MANUS, csv_bases: Dict[str, Path] = CSV_BASES) -> pd.DataFrame:\n",
    "    df_target = cargar_csv(csv_target)\n",
    "\n",
    "    df_target['Remitente'] = df_target['Remitente'].map(norm_nombre)\n",
    "    df_target['Cuerpo'] = df_target['Cuerpo'].map(norm_cuerpo)\n",
    "    df_target['DNI'] = df_target['DNI'].map(norm_num)\n",
    "    df_target['CUIT_CUIL'] = df_target['CUIT_CUIL'].map(norm_num)\n",
    "\n",
    "    resultados = []\n",
    "    for nombre_base, ruta in csv_bases.items():\n",
    "        df_base = cargar_csv(ruta)\n",
    "        df_base['Remitente'] = df_base['Remitente'].map(norm_nombre)\n",
    "        df_base['Cuerpo'] = df_base['Cuerpo'].map(norm_cuerpo)\n",
    "        df_base['DNI'] = df_base['DNI'].map(norm_num)\n",
    "        df_base['CUIT_CUIL'] = df_base['CUIT_CUIL'].map(norm_num)\n",
    "\n",
    "        df_cmp = comparar_base_con_target(df_target, df_base, nombre_base)\n",
    "        if not df_cmp.empty:\n",
    "            resultados.append(df_cmp)\n",
    "\n",
    "    cols = ['ARCHIVO_TARGET','ARCHIVO_BASE','archivo_norm','base',\n",
    "            'dist_remitente','dist_cuerpo','dist_cuerpo_norm','dist_dni','dist_cuit']\n",
    "    if not resultados:\n",
    "        return pd.DataFrame(columns=cols)\n",
    "\n",
    "    out = pd.concat(resultados, ignore_index=True)[cols]\n",
    "    out = out.sort_values(['ARCHIVO_TARGET','base'], kind='mergesort')\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f42376",
   "metadata": {},
   "source": [
    "Ejecutamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0f16d014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR: d:\\Formación\\Managment & Analytics - ITBA\\15. Deep Learning\\Trabajo_Final\n",
      "Target: d:\\Formación\\Managment & Analytics - ITBA\\15. Deep Learning\\Trabajo_Final\\entidades_extraidas_manuscritas.csv -> OK\n",
      "Base DOCTR: d:\\Formación\\Managment & Analytics - ITBA\\15. Deep Learning\\Trabajo_Final\\entidades_extraidas_DOCTR.csv -> OK\n",
      "Base GPT-5: d:\\Formación\\Managment & Analytics - ITBA\\15. Deep Learning\\Trabajo_Final\\entidades_extraidas_GPT-5.csv -> OK\n",
      "Base Tesseract: d:\\Formación\\Managment & Analytics - ITBA\\15. Deep Learning\\Trabajo_Final\\entidades_extraidas_Tesseract.csv -> OK\n",
      "\n",
      "[OK] Comparadas 3 bases contra target manuscritas, sobre 7 archivos.\n",
      "         ARCHIVO_TARGET            ARCHIVO_BASE       archivo_norm      base  dist_remitente  dist_cuerpo  dist_cuerpo_norm  dist_dni  dist_cuit\n",
      "      Bruno23689270.txt       Bruno23689270.txt      bruno23689270     DOCTR               3          897          0.306248      <NA>       <NA>\n",
      "      Bruno23689270.txt       Bruno23689270.txt      bruno23689270     GPT-5              15          403          0.164289         3       <NA>\n",
      "      Bruno23689270.txt       Bruno23689270.txt      bruno23689270 Tesseract               3          317          0.130668      <NA>       <NA>\n",
      "  Bruno400212294002.txt   Bruno400212294002.txt  bruno400212294002     DOCTR               7           34          0.017989         4       <NA>\n",
      "  Bruno400212294002.txt   Bruno400212294002.txt  bruno400212294002     GPT-5               0           46           0.02439         4       <NA>\n",
      "  Bruno400212294002.txt   Bruno400212294002.txt  bruno400212294002 Tesseract              15           10          0.005368      <NA>       <NA>\n",
      "Bruno400212294002_1.txt Bruno400212294002_1.txt bruno4002122940021     DOCTR              29           14          0.005484      <NA>       <NA>\n",
      "Bruno400212294002_1.txt Bruno400212294002_1.txt bruno4002122940021     GPT-5               0          862          0.337774      <NA>       <NA>\n",
      "Bruno400212294002_1.txt Bruno400212294002_1.txt bruno4002122940021 Tesseract               0           15          0.005864      <NA>       <NA>\n",
      "  Coyle401547680601.txt   Coyle401547680601.txt  coyle401547680601     DOCTR               0          210          0.074126      <NA>       <NA>\n",
      "  Coyle401547680601.txt   Coyle401547680601.txt  coyle401547680601     GPT-5              21         1972          0.715789      <NA>       <NA>\n",
      "  Coyle401547680601.txt   Coyle401547680601.txt  coyle401547680601 Tesseract               0          141          0.050339      <NA>       <NA>\n",
      " EVERTEC30707869484.txt                    <NA> evertec30707869484     DOCTR            <NA>         <NA>              <NA>      <NA>       <NA>\n",
      " EVERTEC30707869484.txt  EVERTEC30707869484.txt evertec30707869484     GPT-5               0         6293           0.86788      <NA>          9\n",
      " EVERTEC30707869484.txt  EVERTEC30707869484.txt evertec30707869484 Tesseract              12         6417          0.884981      <NA>       <NA>\n",
      "      Mallo12980371.txt       Mallo12980371.txt      mallo12980371     DOCTR               0         3094          0.857539      <NA>       <NA>\n",
      "      Mallo12980371.txt       Mallo12980371.txt      mallo12980371     GPT-5               0         1139           0.73913         3       <NA>\n",
      "      Mallo12980371.txt       Mallo12980371.txt      mallo12980371 Tesseract              29         1572          0.795949      <NA>       <NA>\n",
      "    Pantano28462989.txt     Pantano28462989.txt    pantano28462989     DOCTR               0           14          0.008578      <NA>       <NA>\n",
      "    Pantano28462989.txt     Pantano28462989.txt    pantano28462989     GPT-5               0          108          0.066543         0       <NA>\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"BASE_DIR:\", BASE_DIR)\n",
    "    print(\"Target:\", CSV_TARGET_MANUS, \"->\", \"OK\" if CSV_TARGET_MANUS.exists() else \"NO\")\n",
    "    for k, v in CSV_BASES.items():\n",
    "        print(f\"Base {k}:\", v, \"->\", \"OK\" if v.exists() else \"NO\")\n",
    "\n",
    "    df_out = comparar_bases_vs_target(CSV_TARGET_MANUS, CSV_BASES)\n",
    "    print(f\"\\n[OK] Comparadas {df_out['base'].nunique()} bases contra target manuscritas, sobre {df_out['ARCHIVO_TARGET'].nunique()} archivos.\")\n",
    "    print(df_out.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0c0175",
   "metadata": {},
   "source": [
    "Vamos a calcular un precisión global por modelo. \n",
    "Tomamos un umbral de dist_cuerpo_norm <= 0.10\n",
    "Es decir, hasta un 10 % de diferencia respecto al manuscrito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "003a9384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        base  total  aciertos  precision_global\n",
      "0      DOCTR      7         4              57.1\n",
      "1      GPT-5      7         2              28.6\n",
      "2  Tesseract      7         4              57.1\n"
     ]
    }
   ],
   "source": [
    "def precision_global_cuerpo(df_out: pd.DataFrame, umbral: float = 0.10) -> pd.DataFrame:\n",
    "    df = df_out.copy()\n",
    "    df['acierto_cuerpo'] = df['dist_cuerpo_norm'].le(umbral)\n",
    "    \n",
    "    return (\n",
    "        df.groupby('base', dropna=False)\n",
    "          .agg(\n",
    "              total=('acierto_cuerpo', 'size'),\n",
    "              aciertos=('acierto_cuerpo', 'sum')\n",
    "          )\n",
    "          .assign(\n",
    "              precision_global=lambda d: (d['aciertos'] / d['total'] * 100).round(1)\n",
    "          )\n",
    "          .reset_index()\n",
    "    )\n",
    "\n",
    "# Ejemplo de uso:\n",
    "precision_cuerpos = precision_global_cuerpo(df_out)\n",
    "print(precision_cuerpos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84ac077",
   "metadata": {},
   "source": [
    "## Resultados\n",
    "La comparación de las 7 cartas manuscritas contra las salidas de DOCTR, GPT‑5 y Tesseract muestra comportamientos heterogéneos en la extracción de texto y datos.\n",
    "\n",
    "1. **Cobertura**: salvo un caso (EVERTEC30707869484), todas las cartas manuscritas tienen correspondencia en las tres bases OCR. La ausencia de match en DOCTR para esta carta fue porque el modelo se exedió del timeout.\n",
    "\n",
    "2. **Remitente**: GPT-5 obtiene distancia cero (mejor coincidencia posible) en 3 cartas (Bruno400212294002.txt, Bruno400212294002_1.txt, Pantano28462989.txt). No obstante, esto ocurre porque en el archivo de target y en GPT estos valores están en NaN (vacíos).\n",
    "\n",
    "3. **Distancias en cuerpo**: Tesseract es el más consistente, ya que obtiene la menor distancia normalizada en 4 de las 6 cartas con datos (Bruno23689270.txt, Bruno400212294002.txt, Bruno400212294002_1.txt, Coyle401547680601.txt). DOCTR tiene mejor desempeño en Pantano28462989.txt (0.0086 vs 0.0665 de GPT-5). GPT-5 sólo logra liderar en una carta (Mallo12980371.txt frente a DOCTR y Tesseract), pero en general su distancia normalizada es más alta, lo que indica menor coincidencias.\n",
    "\n",
    "4. **DNI y CUIT**: gran parte de las comparaciones no poseen valores en estos campos, lo que limita la evaluación de exactitud. \n",
    "\n",
    "## **Conclusión** \n",
    "DOCTR y Tesseract logran, en general, mejor alineación con las cartas manuscritas en remitente y cuerpo, con distancias más bajas y consistentes. GPT‑5, si bien acierta en ciertos campos, presenta variabilidad alta y errores significativos en el cuerpo en varios documentos. Este resultado sugiere que, para manuscritos de este tipo, los OCR clásicos bien configurados parecieran ofrecer salidas más consistentes que la transcripción vía modelo de lenguaje, especialmente en documentos con formato y legibilidad más comprometida.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
