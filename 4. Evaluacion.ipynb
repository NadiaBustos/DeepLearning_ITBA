{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3db171ee",
   "metadata": {},
   "source": [
    "## Evaluación con distancia de Levenshtein.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c6a08f",
   "metadata": {},
   "source": [
    "#### Construcción del target\n",
    " Antes de abordar el problema de la evaluación de los outputs, vamos a extraer entidades de los achivos transcriptos a mano. De esa manera, construimos nuestro target para poder comparar. Utilizamos la misma lógica de extracción de entidades que en los otros casos y usamos el modelo Llama 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34bf8663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "from pydantic import ValidationError\n",
    "import re, unicodedata\n",
    "from tqdm import tqdm\n",
    "from requests.exceptions import ReadTimeout\n",
    "from pathlib import Path\n",
    "import json\n",
    "import csv\n",
    "import requests\n",
    "from requests.exceptions import ReadTimeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00f9d47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TXT_FOLDER = r\"D:\\Formación\\Managment & Analytics - ITBA\\15. Deep Learning\\CDs_Ejemplo\\Transcripciones_manual\"\n",
    "OUTPUT_CSV = \"entidades_extraidas_mauscritas.csv\"\n",
    "PROMPT_FILE = \"prompt_2.txt\"\n",
    "MODEL       = \"llama3:latest\"\n",
    "OLLAMA_URL  = \"http://localhost:11434/api/generate\"\n",
    "TEMPERATURE = 0.0\n",
    "TIMEOUT     = 800\n",
    "MAX_CHARS   = 9000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b4eeb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entity(BaseModel):\n",
    "    Remitente: Optional[str] = None\n",
    "    DNI: Optional[str] = None\n",
    "    CUIT_CUIL: Optional[str] = None\n",
    "    Cuerpo: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9efc6762",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_RE = re.compile(r\"[^\\w\\s.,;:()@€$%/-]\")\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "    s = CLEAN_RE.sub(\"\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29d9e1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"Remitente\": {\"type\": [\"string\", \"null\"]},\n",
    "        \"DNI\": {\"type\": [\"string\", \"null\"], \"pattern\": \"^\\\\d*$\"},\n",
    "        \"CUIT_CUIL\": {\"type\": [\"string\", \"null\"], \"pattern\": \"^\\\\d*$\"},\n",
    "        \"Cuerpo\": {\"type\": \"string\"},\n",
    "    },\n",
    "    \"required\": [\"Cuerpo\"],\n",
    "    \"additionalProperties\": False,\n",
    "}\n",
    "\n",
    "with open(PROMPT_FILE, encoding=\"utf-8\") as f:\n",
    "    PROMPT = f.read()\n",
    "\n",
    "def call_ollama(text: str) -> dict:\n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"prompt\": f\"<|system|>\\n{PROMPT}<|end|>\\n<|user|>\\n{text[:MAX_CHARS]}<|end|>\\n<|assistant|>\",\n",
    "        \"format\": \"json\",\n",
    "        \"stream\": False,\n",
    "        \"options\": {\"temperature\": TEMPERATURE, \"json_schema\": JSON_SCHEMA},\n",
    "    }\n",
    "    try:\n",
    "        r = requests.post(OLLAMA_URL, json=payload, timeout=TIMEOUT)\n",
    "    except ReadTimeout:\n",
    "        raise TimeoutError(\"Timeout de Ollama\")\n",
    "    r.raise_for_status()\n",
    "    return json.loads(r.json()[\"response\"])\n",
    "\n",
    "def main():\n",
    "    dir_path = Path(TXT_FOLDER)\n",
    "    if not dir_path.is_dir():\n",
    "        raise SystemExit(f\"Ruta no encontrada: {dir_path}\")\n",
    "\n",
    "    rows = []\n",
    "    for file in tqdm(sorted(dir_path.glob(\"*.txt\")), desc=\"TXT\"):\n",
    "        raw = file.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        cleaned = clean_text(raw)\n",
    "        try:\n",
    "            data = call_ollama(cleaned)\n",
    "            ent = Entity.model_validate(data)\n",
    "            rows.append({\n",
    "                \"ARCHIVO\": file.name,\n",
    "                \"Remitente\": (ent.Remitente or \"NaN\").strip() or \"NaN\",\n",
    "                \"DNI\": re.sub(r\"\\D\", \"\", ent.DNI or \"\") or \"NaN\",\n",
    "                \"CUIT_CUIL\": re.sub(r\"\\D\", \"\", ent.CUIT_CUIL or \"\") or \"NaN\",\n",
    "                \"Cuerpo\": ent.Cuerpo.strip() or \"NaN\",\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {file.name}: {e}\")\n",
    "    if rows:\n",
    "        with open(OUTPUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            csv.DictWriter(f, fieldnames=rows[0].keys()).writeheader(); csv.DictWriter(f, fieldnames=rows[0].keys()).writerows(rows)\n",
    "        print(f\"✅ CSV generado en {OUTPUT_CSV} ({len(rows)} filas)\")\n",
    "    else:\n",
    "        print(\"No se extrajeron entidades válidas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d0b02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TXT:  57%|█████▋    | 4/7 [20:06<14:28, 289.53s/it]"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efefc83a",
   "metadata": {},
   "source": [
    "### Evalaución\n",
    "\n",
    "Una vez realizada la estracción de entidades con cada método, vamos a analizar cual funcionó mejor. Para ello, vamos a utilizar la **distancia de Levenshtein**. Esta medida indica cuántas operaciones mínimas se necesitan para transformar una cadena en otra. Las operaciones posibles son:\n",
    " - Inserción de un carácter.\n",
    " - Eliminación de un carácter.\n",
    " - Sustitución de un carácter por otro.\n",
    "\n",
    "Por lo tanto, obtener un valor cero significa coincidencia exacta. Cuanto mayor sea el número, más diferentes son las cadenas.\n",
    "\n",
    "Para cada archivo CSV:\n",
    "- Normalizamos el nombre y el contenido para evitar diferencias por formato, mayúsculas o caracteres especiales.\n",
    "- Extraemos variables clave: cuerpo de la carta, DNI y CUIT.\n",
    "- Validamos el CUIT mediante el algoritmo de dígito verificador (módulo 11).\n",
    "- Buscamos el archivo correspondiente en cada CSV usando coincidencia por nombre normalizado.\n",
    "- Comparamos el contenido del TXT con el CSV calculando la distancia de Levenshtein.\n",
    "\n",
    "La salida final muestra, para cada archivo de la base, las distancias entre lo que dice la base y lo que dice cada target.\n",
    "Los valores van de 0 a 1, donde 0 = idéntico y 1 = completamente distinto\n",
    "\n",
    "La ventaja de usar Levenshtein es que detecta pequeñas diferencias (errores de OCR, faltas de ortografía, caracteres faltantes) sin marcar el texto como “completamente distinto”, lo que es clave cuando se trabaja con documentos escaneados y reconocimiento de texto.\n",
    "\n",
    "Importante: Para correr esta parte del script tenés que instalar jellyfish en el entorno con *pip install pandas jellyfish*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beabc39d",
   "metadata": {},
   "source": [
    "1. Instalamos librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31957722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import sys\n",
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import jellyfish\n",
    "except ModuleNotFoundError:\n",
    "    raise SystemExit(\n",
    "        f\"[ERROR] jellyfish no está instalado en este intérprete:\\n  {sys.executable}\\n\"\n",
    "        f\"Instalá con:\\n  \\\"{sys.executable}\\\" -m pip install jellyfish\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04412bc5",
   "metadata": {},
   "source": [
    "2. Definimos el path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19f3ff15",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = r\"D:\\Formación\\Managment & Analytics - ITBA\\15. Deep Learning\\CDs_Ejemplo\"\n",
    "CSV_TARGET_MANUS = Path(BASE_DIR) / \"entidades_extraidas_mauscritas.csv\"  # <- target\n",
    "CSV_BASES = {  # <- bases (OCR)\n",
    "    \"DOCTR\": Path(BASE_DIR) / \"entidades_extraidas_DOCTR.csv\",\n",
    "    \"GPT\": Path(BASE_DIR) / \"entidades_extraidas_GPT.csv\",\n",
    "    \"Tesseract\": Path(BASE_DIR) / \"entidades_extraidas_Tesseract.csv\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754a9485",
   "metadata": {},
   "source": [
    "3. Definimos funciones de normalización para cada variable, para hacerlas más comparables entre sí.\n",
    "\n",
    "    **1) `quitar_acentos(text)`**\n",
    "\n",
    "    * Convierte el texto a forma Unicode **NFD** y quita los caracteres de marca diacrítica (`Mn`).\n",
    "    * Resultado: el string **sin tildes/acentos**.\n",
    "\n",
    "    **2) Variable cuerpo: `norm_cuerpo(x)`**\n",
    "\n",
    "    * Normaliza texto de **cuerpo** para comparaciones:\n",
    "    a) quita acentos → `lower()`\n",
    "    b) elimina todo lo que **no sea** letras minúsculas `a–z`, dígitos `0–9` o espacio (`\\s`) mediante regex `[^a-z0-9\\s]`\n",
    "    c) colapsa espacios múltiples a **uno**.\n",
    "\n",
    "\n",
    "    **3) Variable nombre: `norm_nombre(x)`**\n",
    "\n",
    "    * Normaliza **nombres** (Remitente, por ejemplo):\n",
    "    a) quita acentos → `lower()`\n",
    "    b) **deja solo letras** y espacios (`[^a-z\\s]`)\n",
    "    c) colapsa espacios.\n",
    "\n",
    "\n",
    "    **4)VARIABLES DNI y CUIT_CUIL:`norm_num(x)`**\n",
    "    * Normaliza **números** (DNI, CUIT):\n",
    "    a) convierte a `str`, hace `strip()`\n",
    "    b) si termina en `.0` (típico de CSV/Excel), lo recorta\n",
    "    c) remueve **todo lo que no sea dígito** con `[^0-9]`\n",
    "    d) devuelve `None` si quedó vacío.\n",
    "\n",
    "    **5) `norm_fname(s)`**\n",
    "\n",
    "    * Genera la **clave de emparejamiento** por nombre de archivo:\n",
    "    a) toma solo el **nombre** (`Path(...).name`) y el **stem** (sin extensión)\n",
    "    b) quita acentos, pasa a minúsculas\n",
    "    c) elimina **todo** lo que no sea `a–z` o `0–9`.\n",
    "\n",
    "    **6) `lev(a, b)`**\n",
    "\n",
    "    * Calcula la **distancia de Levenshtein** entre `a` y `b` usando **jellyfish**.\n",
    "    * Si alguno es `None`, devuelve `None`.\n",
    "    * 0 = idéntico; valores mayores = más diferencias.\n",
    "    * Se usa sobre las **formas normalizadas** (de arriba) para comparaciones justas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9079d47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quitar_acentos(text: str) -> str:\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    return \"\".join(c for c in unicodedata.normalize(\"NFD\", str(text)) if unicodedata.category(c) != \"Mn\")\n",
    "\n",
    "\n",
    "def norm_cuerpo(x: str) -> str:\n",
    "    if not x:\n",
    "        return \"\"\n",
    "    t = quitar_acentos(x).lower()\n",
    "    t = re.sub(r\"[^a-z0-9\\s]\", \"\", t)\n",
    "    return \" \".join(t.split())\n",
    "\n",
    "\n",
    "def norm_nombre(x: str) -> str:\n",
    "    if not x:\n",
    "        return \"\"\n",
    "    t = quitar_acentos(x).lower()\n",
    "    t = re.sub(r\"[^a-z\\s]\", \"\", t)\n",
    "    return \" \".join(t.split())\n",
    "\n",
    "\n",
    "def norm_num(x) -> Optional[str]:\n",
    "    if x is None:\n",
    "        return None\n",
    "    s = str(x).strip()\n",
    "    if s.endswith('.0'):\n",
    "        s = s[:-2]\n",
    "    s = re.sub(r\"[^0-9]\", \"\", s)\n",
    "    return s if s else None\n",
    "\n",
    "\n",
    "def norm_fname(s: str) -> str:\n",
    "    \"\"\"Clave de match:** stem** sin acentos, minúsculas, sólo [a-z0-9].\"\"\"\n",
    "    base = Path(str(s)).name\n",
    "    stem = Path(base).stem\n",
    "    t = quitar_acentos(stem).lower()\n",
    "    return re.sub(r\"[^a-z0-9]\", \"\", t)\n",
    "\n",
    "\n",
    "def lev(a: Optional[str], b: Optional[str]) -> Optional[int]:\n",
    "    if a is None or b is None:\n",
    "        return None\n",
    "    return jellyfish.levenshtein_distance(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a035f6",
   "metadata": {},
   "source": [
    "Funciones de lectura: Estas dos funciones homogeneizan los CSV de entrada y construyen una clave de emparejamiento por nombre de archivo. Esto permite que todos los datasets compartan el mismo esquema de columnas y una clave (archivo_norm) comparable entre sí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fc707cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estandarizar_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.rename(columns={c: (c.strip() if isinstance(c, str) else c) for c in df.columns})\n",
    "    lower = {str(c).lower(): c for c in df.columns}\n",
    "\n",
    "    if 'archivo' in lower:\n",
    "        df = df.rename(columns={lower['archivo']: 'archivo'})\n",
    "    elif 'ARCHIVO' in df.columns:\n",
    "        df = df.rename(columns={'ARCHIVO': 'archivo'})\n",
    "\n",
    "    if 'remitente' in lower and 'Remitente' not in df.columns:\n",
    "        df = df.rename(columns={lower['remitente']: 'Remitente'})\n",
    "    if 'cuerpo' in lower and 'Cuerpo' not in df.columns:\n",
    "        df = df.rename(columns={lower['cuerpo']: 'Cuerpo'})\n",
    "    if 'dni' in lower and 'DNI' not in df.columns:\n",
    "        df = df.rename(columns={lower['dni']: 'DNI'})\n",
    "    for k in ['cuit_cuil','cuitcuil','cuit','cuil']:\n",
    "        if k in lower and 'CUIT_CUIL' not in df.columns:\n",
    "            df = df.rename(columns={lower[k]: 'CUIT_CUIL'})\n",
    "            break\n",
    "    if 'CUIT_CUIL' not in df.columns and 'Cuit_Cuil' in df.columns:\n",
    "        df = df.rename(columns={'Cuit_Cuil': 'CUIT_CUIL'})\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def cargar_csv(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, encoding='utf-8-sig', dtype=str)\n",
    "    df = estandarizar_cols(df)\n",
    "    req = {'archivo','Remitente','DNI','CUIT_CUIL','Cuerpo'}\n",
    "    faltan = req - set(df.columns)\n",
    "    if faltan:\n",
    "        raise ValueError(f\"Faltan columnas en {path.name}: {faltan}. Encontradas: {list(df.columns)}\")\n",
    "    # limpiar filas sin archivo\n",
    "    df = df[df['archivo'].notna() & (df['archivo'].astype(str).str.strip()!='')].copy()\n",
    "    df['archivo_norm'] = df['archivo'].map(norm_fname)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde5fc75",
   "metadata": {},
   "source": [
    "Esta función compara fila a fila, un dataset base (p. ej. DOCTR/GPT/Tesseract) contra el target (manuscritas) y calculaa distancias de Levenshtein por campo: Remitente, Cuerpo, DNI, CUIT_CUIL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a292eb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparar_base_con_target(df_base: pd.DataFrame, df_target: pd.DataFrame, nombre_base: str) -> pd.DataFrame:\n",
    "    \"\"\"Left join: mantenemos **todas las filas de la base** y buscamos su par en el target.\"\"\"\n",
    "    b = df_base[['archivo','Remitente','DNI','CUIT_CUIL','Cuerpo','archivo_norm']].copy()\n",
    "    t = df_target[['Remitente','DNI','CUIT_CUIL','Cuerpo','archivo_norm']].copy()\n",
    "\n",
    "    m = b.merge(t, on='archivo_norm', how='left', suffixes=('_base','_tgt'))\n",
    "\n",
    "    dist_rem, dist_cue, dist_cue_norm, dist_dni, dist_cuit = [], [], [], [], []\n",
    "    for _, r in m.iterrows():\n",
    "        # Remitente\n",
    "        rb = norm_nombre(r.get('Remitente_base'))\n",
    "        rt = norm_nombre(r.get('Remitente_tgt')) if pd.notna(r.get('Remitente_tgt')) else None\n",
    "        drem = lev(rb, rt); dist_rem.append(pd.NA if drem is None else drem)\n",
    "        # Cuerpo\n",
    "        cb = norm_cuerpo(r.get('Cuerpo_base'))\n",
    "        ct = norm_cuerpo(r.get('Cuerpo_tgt')) if pd.notna(r.get('Cuerpo_tgt')) else None\n",
    "        dcue = lev(cb, ct); dist_cue.append(pd.NA if dcue is None else dcue)\n",
    "        if ct is None:\n",
    "            dist_cue_norm.append(pd.NA)\n",
    "        else:\n",
    "            mlen = max(len(cb), len(ct)); dist_cue_norm.append(pd.NA if mlen==0 else dcue/mlen)\n",
    "        # DNI\n",
    "        db = norm_num(r.get('DNI_base'))\n",
    "        dt = norm_num(r.get('DNI_tgt')) if pd.notna(r.get('DNI_tgt')) else None\n",
    "        ddni = lev(db, dt); dist_dni.append(pd.NA if ddni is None else ddni)\n",
    "        # CUIT\n",
    "        qb = norm_num(r.get('CUIT_CUIL_base'))\n",
    "        qt = norm_num(r.get('CUIT_CUIL_tgt')) if pd.notna(r.get('CUIT_CUIL_tgt')) else None\n",
    "        dcui = lev(qb, qt); dist_cuit.append(pd.NA if dcui is None else dcui)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        'ARCHIVO': m['archivo'],\n",
    "        'base': nombre_base,  # <- DOCTR/GPT/Tesseract\n",
    "        'dist_remitente': pd.Series(dist_rem, dtype='Int64'),\n",
    "        'dist_cuerpo': pd.Series(dist_cue, dtype='Int64'),\n",
    "        'dist_cuerpo_norm': pd.Series(dist_cue_norm, dtype='Float64'),\n",
    "        'dist_dni': pd.Series(dist_dni, dtype='Int64'),\n",
    "        'dist_cuit': pd.Series(dist_cuit, dtype='Int64'),\n",
    "    })\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7b9398",
   "metadata": {},
   "source": [
    "Definimos el orquetador de las funciones y ejecutamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6985d96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Comparadas 3 bases contra target manuscritas, sobre 13 archivos.\n",
      "                ARCHIVO      base  dist_remitente  dist_cuerpo  dist_cuerpo_norm  dist_dni  dist_cuit\n",
      "       Belen_payway.txt     DOCTR            <NA>         <NA>              <NA>      <NA>       <NA>\n",
      "       Belen_payway.txt       GPT            <NA>         <NA>              <NA>      <NA>       <NA>\n",
      "       Belen_payway.txt Tesseract            <NA>         <NA>              <NA>      <NA>       <NA>\n",
      "      Bruno23689270.txt     DOCTR               3          875          0.300791      <NA>       <NA>\n",
      "      Bruno23689270.txt       GPT              15          624          0.274165         9       <NA>\n",
      "      Bruno23689270.txt Tesseract               3          306          0.127076      <NA>       <NA>\n",
      "  Bruno400212294002.txt     DOCTR               7           33          0.017638         4       <NA>\n",
      "  Bruno400212294002.txt       GPT               0           39          0.021047         4       <NA>\n",
      "  Bruno400212294002.txt Tesseract              15           10          0.005423      <NA>       <NA>\n",
      "Bruno400212294002_1.txt     DOCTR              29           11          0.004353      <NA>       <NA>\n",
      "Bruno400212294002_1.txt       GPT               0          796          0.315248      <NA>       <NA>\n",
      "Bruno400212294002_1.txt Tesseract               0           14          0.005534      <NA>       <NA>\n",
      "  Coyle401547680601.txt     DOCTR               0          195          0.069643      <NA>       <NA>\n",
      "  Coyle401547680601.txt       GPT              12         1769          0.652527      <NA>       <NA>\n",
      "  Coyle401547680601.txt Tesseract               0          131           0.04755      <NA>       <NA>\n",
      " EVERTEC30707869484.txt       GPT              11         7126          0.999579      <NA>       <NA>\n",
      "   Galicia_Giuseppe.txt     DOCTR            <NA>         <NA>              <NA>      <NA>       <NA>\n",
      "   Galicia_Giuseppe.txt       GPT            <NA>         <NA>              <NA>      <NA>       <NA>\n",
      "   Galicia_Giuseppe.txt Tesseract            <NA>         <NA>              <NA>      <NA>       <NA>\n",
      "      Mallo12980371.txt     DOCTR               0         3027           0.85581      <NA>       <NA>\n"
     ]
    }
   ],
   "source": [
    "def comparar_bases_vs_target(csv_target: Path = CSV_TARGET_MANUS, csv_bases: Dict[str, Path] = CSV_BASES) -> pd.DataFrame:\n",
    "    df_target = cargar_csv(csv_target)\n",
    "    resultados = []\n",
    "    for nombre_base, ruta in csv_bases.items():\n",
    "        df_base = cargar_csv(ruta)\n",
    "        df_cmp = comparar_base_con_target(df_base, df_target, nombre_base)\n",
    "        if not df_cmp.empty:\n",
    "            resultados.append(df_cmp)\n",
    "    if not resultados:\n",
    "        return pd.DataFrame(columns=['ARCHIVO','base','dist_remitente','dist_cuerpo','dist_cuerpo_norm','dist_dni','dist_cuit'])\n",
    "    return pd.concat(resultados, ignore_index=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_out = comparar_bases_vs_target()\n",
    "    # Orden por archivo y base\n",
    "    df_out = df_out.sort_values(['ARCHIVO','base'], kind='mergesort')\n",
    "    print(f\"[OK] Comparadas {df_out['base'].nunique()} bases contra target manuscritas, sobre {df_out['ARCHIVO'].nunique()} archivos.\")\n",
    "    print(df_out.head(20).to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava_win",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
