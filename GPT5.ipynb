{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b7481cd",
   "metadata": {},
   "source": [
    "### **Deep Learning - Maestr√≠a Managment & Analytics**\n",
    "\n",
    "## Transcripci√≥n de texto con GPT5\n",
    "\n",
    "En esta notebook vamos a probar la api de GPT5 para leer un documento y transcribirlo. Para ello, nos vamos a conectar a la API. En mi caso, cree un entorno nuevo para evitar conflictos entre librer√≠as, especialmente opencv. \n",
    "Importante: Ten√©s que configurar la API Key en tu entorno para correr este script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0457ce8",
   "metadata": {},
   "source": [
    "1. Instalamos librer√≠as necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b08820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12e3703e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\conda_envs\\gpt5-ocr\\python.exe\n",
      "OpenCV: 4.12.0\n"
     ]
    }
   ],
   "source": [
    "import sys, cv2, fitz, PIL, numpy, openai\n",
    "print(sys.executable)\n",
    "print(\"OpenCV:\", cv2.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df2a6b9",
   "metadata": {},
   "source": [
    "2. Definimos las rutas y modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9501aac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = r\"D:\\Formaci√≥n\\Managment & Analytics - ITBA\\15. Deep Learning\\CDs_Ejemplo\\CDs_Ejemplo\"\n",
    "OUTPUT_BASE = r\"D:\\Formaci√≥n\\Managment & Analytics - ITBA\\15. Deep Learning\\CDs_Ejemplo\"\n",
    "OUTPUT_DIR = os.path.join(OUTPUT_BASE, \"Extraccion_GPT5\")\n",
    "TMP_DIR = os.path.join(OUTPUT_BASE, \"_tmp_gpt5_preproc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376a992e",
   "metadata": {},
   "source": [
    "# Modelo\n",
    "MODEL = \"gpt-5\"           \n",
    "MAX_OUTPUT_TOKENS = 16000 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7903e6b5",
   "metadata": {},
   "source": [
    "3. Definici√≥n del prompt de transcripci√≥n.\n",
    "\n",
    "Estuvimos haciendo varias pruebas por lo que definimos un flag \"Best-Guess\". Cuando est√° activado (True) si un car√°cter o palabra es dudoso, infiere lo m√°s probable seg√∫n el contexto y marc√° el fragmento con (?). Esto puede apagarse en caso de que consigamos documentos con mejor escaneo o resoluci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49088fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_GUESS = True\n",
    "\n",
    "def build_prompt(best_guess: bool) -> str:\n",
    "    if best_guess:\n",
    "        return (\n",
    "            \"Transcrib√≠ el texto EXACTO de este documento.\\n\"\n",
    "            \"- No reformules ni resumas.\\n\"\n",
    "            \"- Si un car√°cter o palabra es dudoso, INFER√ç lo m√°s probable seg√∫n el contexto y marc√° el fragmento con (?) de forma m√≠nima (ej: 'Rivadavia(?) 1234').\\n\"\n",
    "            \"- Conserv√° saltos de l√≠nea, may√∫sculas, puntuaci√≥n y orden.\\n\"\n",
    "            \"- No agregues comentarios ni metadatos.\"\n",
    "        )\n",
    "    return (\n",
    "        \"Transcrib√≠ el texto EXACTO de este documento.\\n\"\n",
    "        \"- No reformules ni resumas.\\n\"\n",
    "        \"- Conserv√° saltos de l√≠nea, may√∫sculas, puntuaci√≥n y orden.\\n\"\n",
    "        \"- No agregues comentarios ni metadatos.\\n\"\n",
    "    )\n",
    "\n",
    "PROMPT = build_prompt(BEST_GUESS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26730da3",
   "metadata": {},
   "source": [
    "Tambi√©n definimos un prompt para minimizar posibles rechazos por copyright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685e6059",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MSG = (\n",
    "    \"El usuario proporcion√≥ el archivo adjunto y autoriza su transcripci√≥n exacta para uso propio. \"\n",
    "    \"La transcripci√≥n est√° permitida por ser contenido suministrado por el usuario. \"\n",
    "    \"No rechaces por derechos de autor.\"\n",
    ")\n",
    "\n",
    "MIN_REASONABLE_LEN = 80\n",
    "REFUSAL_SNIPPETS = (\n",
    "    \"no puedo ayudar\", \"i can't help\", \"cannot help\", \"no puedo\", \"rechazar\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9fbb93",
   "metadata": {},
   "source": [
    "4. Definimos algunos par√°metros de preprocesamiento de la imagen para facilitar su lectura. Estos par√°metros est√°n explicados en detalle en el anexo final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627d0429",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PREPROCESSING = True  \n",
    "\n",
    "RENDER = dict(\n",
    "    dpi=350,         \n",
    "    grayscale=True,  # render en escala de grises (menos ruido y tama√±o)\n",
    ")\n",
    "\n",
    "PREPROC = dict(\n",
    "    denoise_h=7,           # 0 desactiva; 5‚Äì12 reduce grano (fastNlMeans)\n",
    "    clahe_clip=2.0,        # la imagen se divide en una rejilla de mosaicos (tiles y el contraste se ajusta independientemente en cada mosaico.\n",
    "    tile_grid=8,           # tama√±o de bloque para CLAHE\n",
    "    sharpen_amount=0.6,    # controla cu√°nto ‚Äúenfoque‚Äù extra se aplica a la imagen despu√©s de limpiarla y mejorar el contraste.\n",
    "    threshold=\"adaptive\",  # Define c√≥mo se binariza la imagen (pasar de escala de grises a blanco/negro) durante el preprocesamiento. L\n",
    "    deskew=True,           # corregir inclinaci√≥n leve\n",
    ")\n",
    "\n",
    "KEEP_TEMP_FILES = False    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b91f7b",
   "metadata": {},
   "source": [
    "5. Conexi√≥n a api key de open ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b96c337",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from openai import OpenAI\n",
    "except ImportError:\n",
    "    raise SystemExit(\"Falta 'openai'. Instal√° con: pip install --upgrade openai\")\n",
    "\n",
    "client = OpenAI()  # toma OPENAI_API_KEY del entorno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d95e88",
   "metadata": {},
   "source": [
    "6. Funciones para trabajar con los archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00b1c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_dir(path: str) -> None:\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def list_pdfs(folder: str) -> List[Path]:\n",
    "    p = Path(folder)\n",
    "    return sorted([f for f in p.iterdir() if f.suffix.lower() == \".pdf\" and f.is_file()])\n",
    "\n",
    "def already_done(out_txt: Path) -> bool:\n",
    "    return out_txt.exists() and out_txt.stat().st_size > 0\n",
    "\n",
    "def safe_write_text(path: Path, text: str) -> None:\n",
    "    path.write_text(text, encoding=\"utf-8\", errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782ff80e",
   "metadata": {},
   "source": [
    "Convertimos cada pdf a imagen para procesar con OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b035b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_pdf_to_images(pdf_path: Path, dpi: int = 350, grayscale: bool = True) -> List[np.ndarray]:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    scale = dpi / 72.0\n",
    "    mat = fitz.Matrix(scale, scale)\n",
    "    imgs = []\n",
    "    for page in doc:\n",
    "        pix = page.get_pixmap(matrix=mat, alpha=False)\n",
    "        img_bytes = pix.tobytes(\"png\")\n",
    "        pil = Image.open(io.BytesIO(img_bytes))\n",
    "        if grayscale:\n",
    "            pil = pil.convert(\"L\")  # 8-bit gray\n",
    "        arr = np.array(pil)\n",
    "        if arr.ndim == 2:\n",
    "            pass\n",
    "        else:\n",
    "            arr = cv2.cvtColor(arr, cv2.COLOR_RGB2BGR)\n",
    "        imgs.append(arr)\n",
    "    doc.close()\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95001e55",
   "metadata": {},
   "source": [
    "Ahora vamos a definir algunas funciones para pre- procesar. Entre ellas:\n",
    "- Funci√≥n de aplicaci√≥n de **CLAHE (Contrast Limited Adaptive Histogram Equalization)**. Mejora el contraste de manera local dividiendo la imagen en mosaicos de tile √ó tile p√≠xeles.\n",
    "- Funci√≥n de aplicaci√≥n de **unsharp masking**: Desenfoca la imagen (GaussianBlur), resta el desenfoque de la imagen original para resaltar bordes y luego mezcla resultado con la original usando amount. Esto sirve para aumentar la nitidez de bordes de las letras, quedan m√°s definidas.\n",
    "- Funci√≥n de **threshold**: Convierte a blanco/negro seg√∫n el m√©todo adaptativo. Esto es  binarizaci√≥n local por bloques (mejor en iluminaci√≥n irregular). As√≠ se elimina fondo y realza texto, pero puede perder detalles finos si es muy agresivo.\n",
    "- Funci√≥n **deskew**: Corrige inclinaci√≥n del texto.\n",
    "- **preprocess_image** es el pipeline que orquesta todo\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c954c8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_apply_clahe\u001b[39m(gray: \u001b[43mnp\u001b[49m.ndarray, clip: \u001b[38;5;28mfloat\u001b[39m, tile: \u001b[38;5;28mint\u001b[39m) -> np.ndarray:\n\u001b[32m      2\u001b[39m     clahe = cv2.createCLAHE(clipLimit=clip, tileGridSize=(tile, tile))\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m clahe.apply(gray)\n",
      "\u001b[31mNameError\u001b[39m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def _apply_clahe(gray: np.ndarray, clip: float, tile: int) -> np.ndarray:\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip, tileGridSize=(tile, tile))\n",
    "    return clahe.apply(gray)\n",
    "\n",
    "def _unsharp(gray: np.ndarray, amount: float) -> np.ndarray:\n",
    "    if amount <= 0:\n",
    "        return gray\n",
    "    blur = cv2.GaussianBlur(gray, (0, 0), 1.0)\n",
    "    sharp = cv2.addWeighted(gray, 1 + amount, blur, -amount, 0)\n",
    "    return sharp\n",
    "\n",
    "def _threshold(gray: np.ndarray, mode: str) -> np.ndarray:\n",
    "    if mode == \"none\":\n",
    "        return gray\n",
    "    if mode == \"otsu\":\n",
    "        _, bw = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        return bw\n",
    "    if mode == \"adaptive\":\n",
    "        bw = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 31, 10)\n",
    "        return bw\n",
    "    return gray\n",
    "\n",
    "def _deskew(binary_or_gray: np.ndarray) -> np.ndarray:\n",
    "    if binary_or_gray.ndim == 3:\n",
    "        gray = cv2.cvtColor(binary_or_gray, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = binary_or_gray.copy()\n",
    "    _, bw = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    inv = cv2.bitwise_not(bw)\n",
    "    coords = cv2.findNonZero(inv)\n",
    "    if coords is None or len(coords) < 50:\n",
    "        return binary_or_gray\n",
    "    rect = cv2.minAreaRect(coords)\n",
    "    angle = rect[-1]\n",
    "    if angle < -45:\n",
    "        angle = -(90 + angle)\n",
    "    else:\n",
    "        angle = -angle\n",
    "    if abs(angle) < 0.3 or abs(angle) > 15:\n",
    "        return binary_or_gray\n",
    "    (h, w) = gray.shape[:2]\n",
    "    M = cv2.getRotationMatrix2D((w // 2, h // 2), angle, 1.0)\n",
    "    rotated = cv2.warpAffine(binary_or_gray, M, (w, h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REPLICATE)\n",
    "    return rotated\n",
    "\n",
    "def preprocess_image(img: np.ndarray,\n",
    "                     denoise_h: int = 7,\n",
    "                     clahe_clip: float = 2.0,\n",
    "                     tile_grid: int = 8,\n",
    "                     sharpen_amount: float = 0.6,\n",
    "                     threshold: str = \"adaptive\",\n",
    "                     deskew: bool = True) -> np.ndarray:\n",
    "    if img.ndim == 3:\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = img\n",
    "    if denoise_h and denoise_h > 0:\n",
    "        gray = cv2.fastNlMeansDenoising(gray, None, h=denoise_h, templateWindowSize=7, searchWindowSize=21)\n",
    "    gray = _apply_clahe(gray, clip=clahe_clip, tile=tile_grid)\n",
    "    gray = _unsharp(gray, amount=sharpen_amount)\n",
    "    out = _threshold(gray, mode=threshold)\n",
    "    if deskew:\n",
    "        out = _deskew(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d676d4ef",
   "metadata": {},
   "source": [
    "En este bloque vamos a definir una serie de funciones que sirven para crear un auto-selector de preprocesamiento. Esto sirve para crear varias versiones del documento, evaluar enfoque, contraste y detalle. Nos quedamos con la imagen m√°s prometedora para OCR. Detalle de las funciones:\n",
    "\n",
    "- Quality_scores(gray_or_bin): Calcula m√©tricas de calidad visual de una imagen.\n",
    "\n",
    "        - lap_var: varianza del Laplaciano ‚Üí mide el nivel de nitidez. Cuanto m√°s alto, m√°s enfocada est√°.\n",
    "        - contrast: desv√≠o est√°ndar de los p√≠xeles ‚Üí mide el contraste global.\n",
    "        - edges: aplica Canny edge detection para detectar bordes.\n",
    "        - edge_density: proporci√≥n de p√≠xeles que son bordes ‚Üí mide nivel de detalle.\n",
    "\n",
    "- Score_for_selection(img): Combina las m√©tricas en un √∫nico puntaje.\n",
    "- Generate_variants(img): Genera tres versiones distintas de la misma imagen para luego elegir la mejor.\n",
    "- Pick_best_variant(variants): Eval√∫a cada variante con _score_for_selection y elige la de mayor puntaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c47199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _quality_scores(gray_or_bin: np.ndarray) -> dict:\n",
    "    if gray_or_bin.ndim == 3:\n",
    "        gray = cv2.cvtColor(gray_or_bin, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = gray_or_bin\n",
    "    lap_var = cv2.Laplacian(gray, cv2.CV_64F).var()          # enfoque\n",
    "    contrast = float(gray.std())                             # contraste global\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "    edge_density = float(np.count_nonzero(edges)) / edges.size\n",
    "    return {\"lap_var\": lap_var, \"contrast\": contrast, \"edge_density\": edge_density}\n",
    "\n",
    "def _score_for_selection(img: np.ndarray) -> float:\n",
    "    q = _quality_scores(img)\n",
    "    return q[\"lap_var\"] + 40.0 * q[\"edge_density\"] + 0.5 * q[\"contrast\"]\n",
    "\n",
    "def generate_variants(img: np.ndarray) -> list:\n",
    "    base = preprocess_image(\n",
    "        img,\n",
    "        denoise_h=int(PREPROC.get(\"denoise_h\", 7)),\n",
    "        clahe_clip=float(PREPROC.get(\"clahe_clip\", 2.0)),\n",
    "        tile_grid=int(PREPROC.get(\"tile_grid\", 8)),\n",
    "        sharpen_amount=float(PREPROC.get(\"sharpen_amount\", 0.6)),\n",
    "        threshold=str(PREPROC.get(\"threshold\", \"adaptive\")),\n",
    "        deskew=bool(PREPROC.get(\"deskew\", True)),\n",
    "    )\n",
    "    hi_contrast = preprocess_image(\n",
    "        img,\n",
    "        denoise_h=max(5, int(PREPROC.get(\"denoise_h\", 7) - 2)),\n",
    "        clahe_clip=3.0,\n",
    "        tile_grid=8,\n",
    "        sharpen_amount=0.5,\n",
    "        threshold=\"otsu\",\n",
    "        deskew=True,\n",
    "    )\n",
    "    if img.ndim == 3:\n",
    "        g = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        g = img\n",
    "    up = cv2.resize(g, None, fx=2.0, fy=2.0, interpolation=cv2.INTER_LANCZOS4)\n",
    "    up = _rl_deblur(up, iterations=4)\n",
    "    up = _apply_clahe(up, clip=2.5, tile=8)\n",
    "    up = _unsharp(up, amount=0.7)\n",
    "    up = _threshold(up, mode=\"adaptive\")\n",
    "    up = _deskew(up)\n",
    "    return [base, hi_contrast, up]\n",
    "\n",
    "def pick_best_variant(variants: list) -> np.ndarray:\n",
    "    scores = [_score_for_selection(v) for v in variants]\n",
    "    return variants[int(np.argmax(scores))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fa72a3",
   "metadata": {},
   "source": [
    "Definimos una funci√≥n para convertir la lista de im√°genes (ya procesadas en memoria como numpy.ndarray) en un √∫nico archivo PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ce9772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_to_pdf(images: List[np.ndarray], out_pdf_path: Path, dpi: int = 350) -> None:\n",
    "    pil_imgs = []\n",
    "    for im in images:\n",
    "        if im.ndim == 2:\n",
    "            pil = Image.fromarray(im)  # 'L'\n",
    "        else:\n",
    "            pil = Image.fromarray(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "        pil_imgs.append(pil.convert(\"L\"))\n",
    "    pil_imgs[0].save(out_pdf_path, \"PDF\", resolution=dpi, save_all=True, append_images=pil_imgs[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99783cb",
   "metadata": {},
   "source": [
    "Nos conectamos con la API de GPT para enviar el PDF y recibir la transcripci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b7063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_pdf(pdf_path: Path):\n",
    "    return client.files.create(file=pdf_path.open(\"rb\"), purpose=\"user_data\")\n",
    "\n",
    "def transcribe_pdf_fileid(file_id: str) -> str:\n",
    "    # Primer intento\n",
    "    resp = client.responses.create(\n",
    "        model=MODEL,\n",
    "        max_output_tokens=MAX_OUTPUT_TOKENS,\n",
    "        input=[\n",
    "            {\"role\": \"system\", \"content\": [{\"type\": \"input_text\", \"text\": SYSTEM_MSG}]},\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"input_text\", \"text\": PROMPT},\n",
    "                {\"type\": \"input_file\", \"file_id\": file_id},\n",
    "            ]},\n",
    "        ],\n",
    "    )\n",
    "    text = resp.output_text or \"\"\n",
    "\n",
    "    # Heur√≠stica de retry si parece rechazo o es demasiado corto\n",
    "    tlow = text.lower()\n",
    "    if (len(text) < MIN_REASONABLE_LEN) or any(s in tlow for s in REFUSAL_SNIPPETS):\n",
    "        alt = (\n",
    "            \"Soy el propietario del archivo adjunto y solicito transcripci√≥n exacta. \"\n",
    "            \"Si un car√°cter es dudoso, infer√≠ lo m√°s probable y marc√° el fragmento con (?). \"\n",
    "            \"No rechaces por derechos de autor; el contenido fue provisto por el usuario.\"\n",
    "        )\n",
    "        resp2 = client.responses.create(\n",
    "            model=MODEL,\n",
    "            max_output_tokens=MAX_OUTPUT_TOKENS,\n",
    "            input=[\n",
    "                {\"role\": \"system\", \"content\": [{\"type\": \"input_text\", \"text\": SYSTEM_MSG}]},\n",
    "                {\"role\": \"user\", \"content\": [\n",
    "                    {\"type\": \"input_text\", \"text\": alt},\n",
    "                    {\"type\": \"input_file\", \"file_id\": file_id},\n",
    "                ]},\n",
    "            ],\n",
    "        )\n",
    "        text2 = resp2.output_text or \"\"\n",
    "        if len(text2) > len(text):\n",
    "            text = text2\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec3f57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pdf_to_temp(original_pdf: Path, render_cfg: dict, tmp_dir: Path) -> Path:\n",
    "    ensure_dir(str(tmp_dir))\n",
    "    pre_pdf = tmp_dir / f\"{original_pdf.stem}_preproc.pdf\"\n",
    "    # 1) Render\n",
    "    images = render_pdf_to_images(original_pdf, dpi=int(render_cfg.get(\"dpi\", 350)), grayscale=bool(render_cfg.get(\"grayscale\", True)))\n",
    "    # 2) Variantes + selecci√≥n\n",
    "    processed = []\n",
    "    for img in images:\n",
    "        vs = generate_variants(img)\n",
    "        best = pick_best_variant(vs)\n",
    "        processed.append(best)\n",
    "    # 3) Reempacar PDF temporal\n",
    "    images_to_pdf(processed, pre_pdf, dpi=int(render_cfg.get(\"dpi\", 350)))\n",
    "    return pre_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606f6385",
   "metadata": {},
   "source": [
    "Definimos el orquestador final que hace el proceso de principio a fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef868c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_folder(in_dir: str, out_dir: str) -> Tuple[int, List[Tuple[str, str]]]:\n",
    "    ensure_dir(out_dir)\n",
    "    ensure_dir(TMP_DIR)\n",
    "    pdfs = list_pdfs(in_dir)\n",
    "    errors: List[Tuple[str, str]] = []\n",
    "    ok = 0\n",
    "    if not pdfs:\n",
    "        print(\"No se encontraron PDFs en:\", in_dir)\n",
    "        return ok, errors\n",
    "    for i, pdf in enumerate(pdfs, 1):\n",
    "        out_txt = Path(out_dir) / (pdf.stem + \".txt\")\n",
    "        if already_done(out_txt):\n",
    "            print(f\"[{i}/{len(pdfs)}] SKIP (ya existe): {out_txt.name}\")\n",
    "            continue\n",
    "        print(f\"[{i}/{len(pdfs)}] Procesando: {pdf.name}\")\n",
    "        uploaded = None\n",
    "        temp_pdf_path = None\n",
    "        try:\n",
    "            src_for_api = pdf\n",
    "            if USE_PREPROCESSING:\n",
    "                temp_pdf_path = preprocess_pdf_to_temp(pdf, render_cfg=RENDER, tmp_dir=Path(TMP_DIR))\n",
    "                src_for_api = temp_pdf_path\n",
    "            uploaded = upload_pdf(src_for_api)\n",
    "            text = transcribe_pdf_fileid(uploaded.id)\n",
    "            safe_write_text(out_txt, text)\n",
    "            ok += 1\n",
    "            print(f\"   ‚Üí OK: {out_txt.name} ({len(text)} chars)\")\n",
    "        except Exception as e:\n",
    "            msg = str(e)\n",
    "            errors.append((pdf.name, msg))\n",
    "            print(f\"   √ó ERROR en {pdf.name}: {msg}\")\n",
    "        finally:\n",
    "            try:\n",
    "                if uploaded is not None:\n",
    "                    client.files.delete(uploaded.id)\n",
    "                    sleep(0.2)\n",
    "            except Exception:\n",
    "                pass\n",
    "            try:\n",
    "                if temp_pdf_path and not KEEP_TEMP_FILES and Path(temp_pdf_path).exists():\n",
    "                    Path(temp_pdf_path).unlink()\n",
    "            except Exception:\n",
    "                pass\n",
    "    return ok, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad69060",
   "metadata": {},
   "source": [
    "7. Ejecutamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73315a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/13] Procesando: Belen_payway.pdf\n",
      "   ‚Üí OK: Belen_payway.txt (3144 chars)\n",
      "[2/13] Procesando: Bruno23689270.pdf\n",
      "   ‚Üí OK: Bruno23689270.txt (2714 chars)\n",
      "[3/13] Procesando: Bruno400212294002.pdf\n",
      "   ‚Üí OK: Bruno400212294002.txt (3031 chars)\n",
      "[4/13] Procesando: Bruno400212294002_1.pdf\n",
      "   ‚Üí OK: Bruno400212294002_1.txt (3059 chars)\n",
      "[5/13] Procesando: Coyle401547680601.pdf\n",
      "   ‚Üí OK: Coyle401547680601.txt (2667 chars)\n",
      "[6/13] Procesando: EVERTEC30707869484.pdf\n",
      "   ‚Üí OK: EVERTEC30707869484.txt (428 chars)\n",
      "[7/13] Procesando: Galicia_Giuseppe.pdf\n",
      "   ‚Üí OK: Galicia_Giuseppe.txt (873 chars)\n",
      "[8/13] Procesando: hsbc_Aquino.pdf\n",
      "   ‚Üí OK: hsbc_Aquino.txt (288 chars)\n",
      "[9/13] Procesando: hsbc_Aquino2.pdf\n",
      "   ‚Üí OK: hsbc_Aquino2.txt (2795 chars)\n",
      "[10/13] Procesando: hsbc_Aquino3.pdf\n",
      "   ‚Üí OK: hsbc_Aquino3.txt (1530 chars)\n",
      "[11/13] Procesando: Mallo12980371.pdf\n",
      "   ‚Üí OK: Mallo12980371.txt (1895 chars)\n",
      "[12/13] Procesando: Pantano28462989.pdf\n",
      "   ‚Üí OK: Pantano28462989.txt (2194 chars)\n",
      "[13/13] Procesando: Santander_evertec.pdf\n",
      "   ‚Üí OK: Santander_evertec.txt (1128 chars)\n",
      "\n",
      "Resumen\n",
      "=======\n",
      "Transcripciones OK: 13\n",
      "\n",
      "TXT guardados en: D:\\Formaci√≥n\\Managment & Analytics - ITBA\\15. Deep Learning\\CDs_Ejemplo\\Extraccion_GPT5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===== Ejecutar =====\n",
    "if __name__ == \"__main__\":\n",
    "    ok, errs = process_folder(INPUT_DIR, OUTPUT_DIR)\n",
    "    print(\"\\nResumen\\n=======\")\n",
    "    print(f\"Transcripciones OK: {ok}\")\n",
    "    if errs:\n",
    "        print(f\"Con errores: {len(errs)}\")\n",
    "        for fname, emsg in errs[:10]:\n",
    "            print(f\" - {fname}: {emsg[:180]}{'...' if len(emsg)>180 else ''}\")\n",
    "        if len(errs) > 10:\n",
    "            print(f\"   (+ {len(errs)-10} errores m√°s)\")\n",
    "    print(f\"\\nTXT guardados en: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651282b7",
   "metadata": {},
   "source": [
    "Otra versi√≥n (borrar cuando tenga todo corrido)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b2619b",
   "metadata": {},
   "source": [
    "1. Configuraci√≥n (no era exacta as√≠, puede que falle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6386cc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROOT_FOLDER = r\"D:\\Formaci√≥n\\Managment & Analytics - ITBA\\15. Deep Learning\\CDs_Ejemplo\\CDs_Ejemplo\"\n",
    "# OUT_DIR_NAME = \"extraccion_gpt5\"\n",
    "# MODEL = \"gpt-5\"          # o \"gpt-5-mini\" si quer√©s menor costo\n",
    "# DPI_TRY = [240, 300, 340]  # escalado progresivo\n",
    "# TILE_GRID = (2, 2)       # fallback: cortar en 2x2\n",
    "# API_SLEEP = 0.8          # pausa m√≠nima entre requests (evita rate limit)\n",
    "# PROMPT = (\n",
    "#     \"TRANSCRIPCION EXACTA obligatoria. Si algo no se distingue, \"\n",
    "#     \"escrib√≠ [ilegible] en ese fragmento, sin rechazar la tarea. \"\n",
    "#     \"Conserv√° may√∫sculas, signos y SALTOS DE L√çNEA del original. \"\n",
    "#     \"No corrijas ortograf√≠a ni formato. Devolv√© SOLO el texto transcrito.\"\n",
    "# )\n",
    "# NEGATION_HINTS = [\n",
    "#     \"no puedo transcribir\", \"no puedo leer\", \"borrosa\", \"ilegible\", \"no se distingue\",\n",
    "#     \"no puedo cumplir\", \"no puedo realizar\", \"no puedo hacerlo\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c310695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ensure_api_key():\n",
    "#     api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "#     if not api_key:\n",
    "#         raise RuntimeError(\n",
    "#             \"Falta OPENAI_API_KEY. En PowerShell: setx OPENAI_API_KEY \\\"tu_api_key\\\" y reabr√≠ la terminal.\"\n",
    "#         )\n",
    "#     return api_key\n",
    "\n",
    "# def make_out_path(pdf_path: str) -> Path:\n",
    "#     pdf = Path(pdf_path)\n",
    "#     out_dir = pdf.parent / OUT_DIR_NAME\n",
    "#     out_dir.mkdir(parents=True, exist_ok=True)\n",
    "#     return out_dir / (pdf.stem + \".txt\")\n",
    "\n",
    "# def page_png_data_url(page, dpi: int) -> str:\n",
    "#     # Render a PNG y devolver como data URL (data:image/png;base64,...)\n",
    "#     zoom = dpi / 72.0\n",
    "#     mat = fitz.Matrix(zoom, zoom)\n",
    "#     pix = page.get_pixmap(matrix=mat, alpha=False)\n",
    "#     b64 = base64.b64encode(pix.tobytes(\"png\")).decode(\"utf-8\")\n",
    "#     return f\"data:image/png;base64,{b64}\"\n",
    "\n",
    "# def transcribe_pdf(pdf_path: str, model: str, dpi: int) -> str:\n",
    "#     client = OpenAI(api_key=ensure_api_key())\n",
    "#     doc = fitz.open(pdf_path)\n",
    "#     outputs = []\n",
    "#     total = len(doc)\n",
    "#     for i, page in enumerate(doc, start=1):\n",
    "#         img_data_url = page_png_data_url(page, dpi)\n",
    "#         resp = client.responses.create(\n",
    "#             model=model,\n",
    "#             input=[{\n",
    "#                 \"role\": \"user\",\n",
    "#                 \"content\": [\n",
    "#                     {\"type\": \"input_text\", \"text\": f\"{PROMPT}\\nP√°gina {i}:\"},\n",
    "#                     {\"type\": \"input_image\", \"image_url\": img_data_url}\n",
    "#                 ]\n",
    "#             }]\n",
    "#         )\n",
    "#         texto = (resp.output_text or \"\").strip()\n",
    "#         outputs.append(f\"--- P√°gina {i} ---\\n{texto}\\n\")\n",
    "#         print(f\"‚úÖ P√°gina {i}/{total} lista\")\n",
    "#     doc.close()\n",
    "#     return \"\\n\".join(outputs).strip()\n",
    "\n",
    "# def main():\n",
    "#     pdf_path = Path(PDF_PATH)\n",
    "#     if not pdf_path.is_file():\n",
    "#         raise FileNotFoundError(f\"No se encontr√≥ el PDF: {pdf_path}\")\n",
    "\n",
    "#     out_txt = make_out_path(str(pdf_path))\n",
    "#     print(f\"Transcribiendo: {pdf_path}\")\n",
    "#     texto = transcribe_pdf(str(pdf_path), MODEL, DPI)\n",
    "#     out_txt.write_text(texto, encoding=\"utf-8\")\n",
    "#     print(f\"\\nüìÑ TXT guardado en: {out_txt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e27a7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribiendo: D:\\Formaci√≥n\\Managment & Analytics - ITBA\\15. Deep Learning\\CDs_Ejemplo\\CDs_Ejemplo\\Belen_payway.pdf\n",
      "‚úÖ P√°gina 1/1 lista\n",
      "\n",
      "üìÑ TXT guardado en: D:\\Formaci√≥n\\Managment & Analytics - ITBA\\15. Deep Learning\\CDs_Ejemplo\\CDs_Ejemplo\\extraccion_gpt5\\Belen_payway.txt\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05bffc7",
   "metadata": {},
   "source": [
    "## Extracci√≥n de variables con Llama 3\n",
    "Vamos a seguir la misma l√≥gica de extracci√≥n de variables que utilizamos con Tesseract y DOCTR. \n",
    "Asegurate de tener instalado en tu entorno los paquetes necesarioas. Si no los ten√©s, corr√© desde powershell: pip install requests tqdm pydantic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "525de83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in d:\\conda_envs\\nn_env\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: tqdm in d:\\conda_envs\\nn_env\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: pydantic in d:\\conda_envs\\nn_env\\lib\\site-packages (2.11.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\conda_envs\\nn_env\\lib\\site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\conda_envs\\nn_env\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\conda_envs\\nn_env\\lib\\site-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\conda_envs\\nn_env\\lib\\site-packages (from requests) (2025.4.26)\n",
      "Requirement already satisfied: colorama in d:\\conda_envs\\nn_env\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\conda_envs\\nn_env\\lib\\site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\conda_envs\\nn_env\\lib\\site-packages (from pydantic) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in d:\\conda_envs\\nn_env\\lib\\site-packages (from pydantic) (4.13.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\conda_envs\\nn_env\\lib\\site-packages (from pydantic) (0.4.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests tqdm pydantic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68b248ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "from pydantic import ValidationError\n",
    "import re, unicodedata\n",
    "from tqdm import tqdm\n",
    "from requests.exceptions import ReadTimeout\n",
    "from pathlib import Path\n",
    "import json\n",
    "import csv\n",
    "import requests\n",
    "from requests.exceptions import ReadTimeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1580caff",
   "metadata": {},
   "outputs": [],
   "source": [
    "TXT_FOLDER = r\"D:\\Formaci√≥n\\Managment & Analytics - ITBA\\15. Deep Learning\\CDs_Ejemplo\\Extraccion_GPT5\"\n",
    "OUTPUT_CSV = \"entidades_extraidas_GPT.csv\"\n",
    "PROMPT_FILE = \"prompt_2.txt\"\n",
    "MODEL       = \"llama3:latest\"\n",
    "OLLAMA_URL  = \"http://localhost:11434/api/generate\"\n",
    "TEMPERATURE = 0.0\n",
    "TIMEOUT     = 800\n",
    "MAX_CHARS   = 9000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1df8582c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entity(BaseModel):\n",
    "    Remitente: Optional[str] = None\n",
    "    DNI: Optional[str] = None\n",
    "    CUIT_CUIL: Optional[str] = None\n",
    "    Cuerpo: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64998613",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_RE = re.compile(r\"[^\\w\\s.,;:()@‚Ç¨$%/-]\")\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "    s = CLEAN_RE.sub(\"\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5aa30655",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"Remitente\": {\"type\": [\"string\", \"null\"]},\n",
    "        \"DNI\": {\"type\": [\"string\", \"null\"], \"pattern\": \"^\\\\d*$\"},\n",
    "        \"CUIT_CUIL\": {\"type\": [\"string\", \"null\"], \"pattern\": \"^\\\\d*$\"},\n",
    "        \"Cuerpo\": {\"type\": \"string\"},\n",
    "    },\n",
    "    \"required\": [\"Cuerpo\"],\n",
    "    \"additionalProperties\": False,\n",
    "}\n",
    "\n",
    "with open(PROMPT_FILE, encoding=\"utf-8\") as f:\n",
    "    PROMPT = f.read()\n",
    "\n",
    "def call_ollama(text: str) -> dict:\n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"prompt\": f\"<|system|>\\n{PROMPT}<|end|>\\n<|user|>\\n{text[:MAX_CHARS]}<|end|>\\n<|assistant|>\",\n",
    "        \"format\": \"json\",\n",
    "        \"stream\": False,\n",
    "        \"options\": {\"temperature\": TEMPERATURE, \"json_schema\": JSON_SCHEMA},\n",
    "    }\n",
    "    try:\n",
    "        r = requests.post(OLLAMA_URL, json=payload, timeout=TIMEOUT)\n",
    "    except ReadTimeout:\n",
    "        raise TimeoutError(\"Timeout de Ollama\")\n",
    "    r.raise_for_status()\n",
    "    return json.loads(r.json()[\"response\"])\n",
    "\n",
    "def main():\n",
    "    dir_path = Path(TXT_FOLDER)\n",
    "    if not dir_path.is_dir():\n",
    "        raise SystemExit(f\"Ruta no encontrada: {dir_path}\")\n",
    "\n",
    "    rows = []\n",
    "    for file in tqdm(sorted(dir_path.glob(\"*.txt\")), desc=\"TXT\"):\n",
    "        raw = file.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        cleaned = clean_text(raw)\n",
    "        try:\n",
    "            data = call_ollama(cleaned)\n",
    "            ent = Entity.model_validate(data)\n",
    "            rows.append({\n",
    "                \"ARCHIVO\": file.name,\n",
    "                \"Remitente\": (ent.Remitente or \"NaN\").strip() or \"NaN\",\n",
    "                \"DNI\": re.sub(r\"\\D\", \"\", ent.DNI or \"\") or \"NaN\",\n",
    "                \"CUIT_CUIL\": re.sub(r\"\\D\", \"\", ent.CUIT_CUIL or \"\") or \"NaN\",\n",
    "                \"Cuerpo\": ent.Cuerpo.strip() or \"NaN\",\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {file.name}: {e}\")\n",
    "    if rows:\n",
    "        with open(OUTPUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            csv.DictWriter(f, fieldnames=rows[0].keys()).writeheader(); csv.DictWriter(f, fieldnames=rows[0].keys()).writerows(rows)\n",
    "        print(f\"‚úÖ CSV generado en {OUTPUT_CSV} ({len(rows)} filas)\")\n",
    "    else:\n",
    "        print(\"No se extrajeron entidades v√°lidas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9743eaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TXT: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [1:44:15<00:00, 481.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CSV generado en entidades_extraidas_GPT.csv (13 filas)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c84746b",
   "metadata": {},
   "source": [
    "## Anexo "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f652dd",
   "metadata": {},
   "source": [
    "En las primeras iteraciones detectamos que hay algunos documentos que el modelo no puede leer. As√≠ que vamos a aplicar un pre-procesamiento con OPENCV\n",
    "\n",
    "\n",
    "| Funci√≥n                                                                         | Qu√© hace                                                              | D√≥nde la usamos en el script                    | Para qu√© sirve                                                                  |\n",
    "| ------------------------------------------------------------------------------- | --------------------------------------------------------------------- | ----------------------------------------------- | ----------------------------------------------------------------------------------------- |\n",
    "| `cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)`                                         | Convierte una imagen a escala de grises.                              | En `preprocess_image` y `render_pdf_to_images`. | Reducir el procesamiento a un solo canal, eliminando color innecesario y ruido crom√°tico. |\n",
    "| `cv2.fastNlMeansDenoising(gray, None, h, templateWindowSize, searchWindowSize)` | Elimina ruido/grano manteniendo bordes.                               | En `preprocess_image`.                          | Mejora la legibilidad de texto escaneado y suprime manchas de fondo.                      |\n",
    "| `cv2.createCLAHE(clipLimit, tileGridSize)` + `.apply(gray)`                     | Ajuste adaptativo de contraste (CLAHE).                               | En `_apply_clahe`.                              | Aumenta contraste local, √∫til cuando partes del documento son muy claras o muy oscuras.   |\n",
    "| `cv2.GaussianBlur(gray, (0, 0), sigma)`                                         | Desenfoque gaussiano.                                                 | En `_unsharp`.                                  | Usado para crear un efecto de ‚Äúresta‚Äù que resalta bordes (sharpening).                    |\n",
    "| `cv2.addWeighted(src1, alpha, src2, beta, gamma)`                               | Combina im√°genes ponderadas.                                          | En `_unsharp`.                                  | Crea el efecto de nitidez a partir del original y el desenfoque.                          |\n",
    "| `cv2.threshold(gray, thresh, maxval, tipo)`                                     | Binarizaci√≥n global (incluye Otsu).                                   | En `_threshold` y `_deskew`.                    | Convierte el documento a blanco y negro, eliminando variaciones de fondo.                 |\n",
    "| `cv2.adaptiveThreshold(gray, maxval, metodo, tipo, blockSize, C)`               | Binarizaci√≥n adaptativa.                                              | En `_threshold`.                                | M√°s robusta en documentos con iluminaci√≥n no uniforme o manchas.                          |\n",
    "| `cv2.bitwise_not(bw)`                                                           | Invierte blanco/negro.                                                | En `_deskew`.                                   | Facilita la detecci√≥n de bordes para estimar inclinaci√≥n.                                 |\n",
    "| `cv2.findNonZero(img)`                                                          | Encuentra todos los p√≠xeles distintos de cero.                        | En `_deskew`.                                   | Detectar el √°rea de texto para estimar el √°ngulo de inclinaci√≥n.                          |\n",
    "| `cv2.minAreaRect(coords)`                                                       | Encuentra el rect√°ngulo de √°rea m√≠nima que contiene todos los puntos. | En `_deskew`.                                   | Calcular √°ngulo del texto en el documento.                                                |\n",
    "| `cv2.getRotationMatrix2D(center, angle, scale)`                                 | Matriz de rotaci√≥n 2D.                                                | En `_deskew`.                                   | Crear la transformaci√≥n para corregir inclinaci√≥n.                                        |\n",
    "| `cv2.warpAffine(img, M, (w, h), flags, borderMode)`                             | Aplica transformaci√≥n af√≠n (rotaci√≥n).                                | En `_deskew`.                                   | Gira la imagen para alinear el texto con el eje horizontal.                               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dee778",
   "metadata": {},
   "source": [
    "**Pipeline general**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f36e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [process_folder]\n",
    "#     ‚îÇ\n",
    "#     ‚îú‚îÄ ensure_dir(out_dir), ensure_dir(TMP_DIR)\n",
    "#     ‚îú‚îÄ pdfs = list_pdfs(in_dir)\n",
    "#     ‚îÇ\n",
    "#     ‚îî‚îÄ for pdf in pdfs:\n",
    "#          ‚îÇ\n",
    "#          ‚îú‚îÄ out_txt = <out_dir>/<pdf.stem>.txt\n",
    "#          ‚îú‚îÄ if already_done(out_txt):  ‚Üí SKIP\n",
    "#          ‚îÇ\n",
    "#          ‚îú‚îÄ if USE_PREPROCESSING:\n",
    "#          ‚îÇ     ‚îÇ\n",
    "#          ‚îÇ     ‚îî‚îÄ [preprocess_pdf_to_temp]\n",
    "#          ‚îÇ          ‚îÇ\n",
    "#          ‚îÇ          ‚îú‚îÄ images = render_pdf_to_images(pdf, dpi, grayscale)\n",
    "#          ‚îÇ          ‚îÇ\n",
    "#          ‚îÇ          ‚îî‚îÄ processed = []\n",
    "#          ‚îÇ                ‚îî‚îÄ for img in images:\n",
    "#          ‚îÇ                      ‚îú‚îÄ variants = generate_variants(img)\n",
    "#          ‚îÇ                      ‚îÇ     ‚îú‚îÄ base = preprocess_image(...)\n",
    "#          ‚îÇ                      ‚îÇ     ‚îú‚îÄ hi_contrast = preprocess_image(...)\n",
    "#          ‚îÇ                      ‚îÇ     ‚îî‚îÄ up = resize+deblur+CLAHE+unsharp+threshold+deskew\n",
    "#          ‚îÇ                      ‚îî‚îÄ best = pick_best_variant(variants)\n",
    "#          ‚îÇ                         (usa _quality_scores: lap_var + edge_density + contrast)\n",
    "#          ‚îÇ                ‚îî‚îÄ images_to_pdf(processed, tmp_preproc.pdf, dpi)\n",
    "#          ‚îÇ\n",
    "#          ‚îú‚îÄ src_for_api = tmp_preproc.pdf  (o pdf original si no hay preproc)\n",
    "#          ‚îÇ\n",
    "#          ‚îú‚îÄ uploaded = upload_pdf(src_for_api)     ‚Üí file_id\n",
    "#          ‚îú‚îÄ text = transcribe_pdf_fileid(file_id)\n",
    "#          ‚îÇ     ‚îú‚îÄ responses.create([\n",
    "#          ‚îÇ     ‚îÇ     {\"role\": \"system\", \"content\": [{\"type\": \"input_text\", \"text\": SYSTEM_MSG}]},\n",
    "#          ‚îÇ     ‚îÇ     {\"role\": \"user\", \"content\": [\n",
    "#          ‚îÇ     ‚îÇ         {\"type\": \"input_text\", \"text\": PROMPT},\n",
    "#          ‚îÇ     ‚îÇ         {\"type\": \"input_file\", \"file_id\": file_id}\n",
    "#          ‚îÇ     ‚îÇ     ]}\n",
    "#          ‚îÇ     ‚îÇ ])\n",
    "#          ‚îÇ     ‚îî‚îÄ retry si corto/refusal ‚Üí prompt alternativo\n",
    "#          ‚îÇ\n",
    "#          ‚îú‚îÄ safe_write_text(out_txt, text)\n",
    "#          ‚îÇ\n",
    "#          ‚îî‚îÄ finally:\n",
    "#                ‚îú‚îÄ client.files.delete(uploaded.id)    # limpia storage remoto\n",
    "#                ‚îî‚îÄ if not KEEP_TEMP_FILES: borrar tmp_preproc.pdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
