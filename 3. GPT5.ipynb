{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b7481cd",
   "metadata": {},
   "source": [
    "## Transcripción de texto con GPT5\n",
    "\n",
    "En esta notebook vamos a probar la api de GPT5 para leer un documento y transcribirlo. En mi caso, cree un entorno nuevo para evitar conflictos entre librerías. Antes de pasarle la imagen al modelo vamos a procesarla con OpenCV (Open Source Computer Vision Library)\n",
    "\n",
    "Importante: Tenés que configurar la API Key en tu entorno para correr este script. También instalar OpenCV, desde la terminal: *pip install opencv-python*\n",
    "\n",
    "Documentación del modelo https://platform.openai.com/docs/guides/latest-model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0457ce8",
   "metadata": {},
   "source": [
    "1. Instalamos librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54b08820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12e3703e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\conda_envs\\gpt5-ocr\\python.exe\n",
      "OpenCV: 4.12.0\n"
     ]
    }
   ],
   "source": [
    "import sys, cv2, PIL, openai\n",
    "print(sys.executable)\n",
    "print(\"OpenCV:\", cv2.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df2a6b9",
   "metadata": {},
   "source": [
    "2. Definimos las rutas y modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11f0317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR= Path(\"cds_ejemplo\")\n",
    "OUTPUT_BASE = Path(\"outputs\") / \"Extraccion_GPT-5\"\n",
    "\n",
    "OUTPUT_BASE.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DIR = os.path.join(OUTPUT_BASE, \"Extraccion_GPT5\")\n",
    "TMP_DIR = os.path.join(OUTPUT_BASE, \"_tmp_gpt5_preproc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e6a1a5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-5\"           \n",
    "MAX_OUTPUT_TOKENS = 16000 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7903e6b5",
   "metadata": {},
   "source": [
    "3. Definición del prompt de transcripción.\n",
    "\n",
    "Para escribir el prompt, tuvimos en cuenta las recomendaciones del Coockbook de Open AI https://cookbook.openai.com/examples/gpt-5/gpt-5_prompting_guide \n",
    "Limitamos el comportamiento a pedidos específicos y claros, con un refuerzo del prompt antes y después de la transcripción. Este modelo viene, por defecto con un verbosity medio (extensión de la respuesta) y también un reasoning_effort medio. Nos pareció adecuado para la tarea a realizar.\n",
    "\n",
    "También definimos un flag \"Best-Guess\". Cuando está activado (True) si un carácter o palabra es dudoso, infiere lo más probable según el contexto y marcá el fragmento con (?). Esto puede apagarse en caso de que consigamos documentos con mejor escaneo o resolución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49088fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_GUESS = True\n",
    "\n",
    "def build_prompt(best_guess: bool) -> str:\n",
    "    if best_guess:\n",
    "        return (\n",
    "            \"Transcribí el texto EXACTO de este documento.\\n\"\n",
    "            \"- No reformules ni resumas.\\n\"\n",
    "            \"- Si un carácter o palabra es dudoso, INFERÍ lo más probable según el contexto y marcá el fragmento con (?) de forma mínima (ej: 'Rivadavia(?) 1234').\\n\"\n",
    "            \"- Conservá saltos de línea, mayúsculas, puntuación y orden.\\n\"\n",
    "            \"- No agregues comentarios ni metadatos.\"\n",
    "        )\n",
    "    return (\n",
    "        \"Transcribí el texto EXACTO de este documento.\\n\"\n",
    "        \"- No reformules ni resumas.\\n\"\n",
    "        \"- Conservá saltos de línea, mayúsculas, puntuación y orden.\\n\"\n",
    "        \"- No agregues comentarios ni metadatos.\\n\"\n",
    "    )\n",
    "\n",
    "PROMPT = build_prompt(BEST_GUESS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26730da3",
   "metadata": {},
   "source": [
    "Luego de varias iteraciones con el prompt, nos dimos cuenta que funciona mejor si definimos un contexto para responder. Por un lado, SYSTEM_MSG le indica que no debe rechazar la solicitud por temas de derechos de autor. Por otro, con MIN_REASONABLE_LEN y REFUSAL_SNIPPETS nos aseguramos de controlar la lógica de pensamiento  para tener una transcripción lo suficientemente larga como para ser válida y que no sea rechazada por dificultades de lectura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "685e6059",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MSG = (\n",
    "    \"El usuario proporcionó el archivo adjunto y autoriza su transcripción exacta para uso propio. \"\n",
    "    \"La transcripción está permitida por ser contenido suministrado por el usuario. \"\n",
    "    \"No rechaces por derechos de autor.\"\n",
    ")\n",
    "\n",
    "MIN_REASONABLE_LEN = 80\n",
    "REFUSAL_SNIPPETS = (\n",
    "    \"no puedo ayudar\", \"i can't help\", \"cannot help\", \"no puedo\", \"rechazar\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9fbb93",
   "metadata": {},
   "source": [
    "4. Definimos algunos parámetros de preprocesamiento de la imagen para facilitar su lectura. Luego, creamos funciones para cada uno.\n",
    "\n",
    "| Parámetro de config    | Función usada en el código                           | Qué hace                                                                     |\n",
    "| ---------------------- | ---------------------------------------------------- | ---------------------------------------------------------------------------- |\n",
    "| `denoise_h=7`          | `cv2.fastNlMeansDenoising(...)`                      | Quita ruido/grano de la imagen.                                              |\n",
    "| `clahe_clip=2.0`       | `_apply_clahe(gray, clip, tile)` → `cv2.createCLAHE` | Mejora contraste localmente usando CLAHE. La imagen se divide en una rejilla de mosaicos y el contraste se ajusta independientemente en cada mosaico.                                    |\n",
    "| `tile_grid=8`          | Pasado como `tile` a `_apply_clahe`                  | Tamaño de mosaico para ajustar contraste.                                    |\n",
    "| `sharpen_amount=0.6`   | `_unsharp(...)`                                      | Aplica enfoque extra combinando imagen original y difuminada (unsharp mask). |\n",
    "| `threshold=\"adaptive\"` | `_threshold(...)`                                    | Convierte a blanco/negro usando umbral adaptativo.                           |\n",
    "| `deskew=True`          | `_deskew(...)`                                       | Corrige inclinación detectando el ángulo de rotación.                        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "627d0429",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PREPROCESSING = True  \n",
    "\n",
    "RENDER = dict(\n",
    "    dpi=350,         \n",
    "    grayscale=True,  # render en escala de grises (menos ruido y tamaño)\n",
    ")\n",
    "\n",
    "PREPROC = dict(\n",
    "    denoise_h=7,           \n",
    "    clahe_clip=2.0,       \n",
    "    tile_grid=8,           \n",
    "    sharpen_amount=0.6,    \n",
    "    threshold=\"adaptive\",  \n",
    "    deskew=True,          \n",
    ")\n",
    "\n",
    "KEEP_TEMP_FILES = False    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5c954c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_clahe(gray: np.ndarray, clip: float, tile: int) -> np.ndarray:\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip, tileGridSize=(tile, tile))\n",
    "    return clahe.apply(gray)\n",
    "\n",
    "def _unsharp(gray: np.ndarray, amount: float) -> np.ndarray:\n",
    "    if amount <= 0:\n",
    "        return gray\n",
    "    blur = cv2.GaussianBlur(gray, (0, 0), 1.0)\n",
    "    sharp = cv2.addWeighted(gray, 1 + amount, blur, -amount, 0)\n",
    "    return sharp\n",
    "\n",
    "def _threshold(gray: np.ndarray, mode: str) -> np.ndarray:\n",
    "    if mode == \"none\":\n",
    "        return gray\n",
    "    if mode == \"otsu\":\n",
    "        _, bw = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        return bw\n",
    "    if mode == \"adaptive\":\n",
    "        bw = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 31, 10)\n",
    "        return bw\n",
    "    return gray\n",
    "\n",
    "def _deskew(binary_or_gray: np.ndarray) -> np.ndarray:\n",
    "    if binary_or_gray.ndim == 3:\n",
    "        gray = cv2.cvtColor(binary_or_gray, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = binary_or_gray.copy()\n",
    "    _, bw = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    inv = cv2.bitwise_not(bw)\n",
    "    coords = cv2.findNonZero(inv)\n",
    "    if coords is None or len(coords) < 50:\n",
    "        return binary_or_gray\n",
    "    rect = cv2.minAreaRect(coords)\n",
    "    angle = rect[-1]\n",
    "    if angle < -45:\n",
    "        angle = -(90 + angle)\n",
    "    else:\n",
    "        angle = -angle\n",
    "    if abs(angle) < 0.3 or abs(angle) > 15:\n",
    "        return binary_or_gray\n",
    "    (h, w) = gray.shape[:2]\n",
    "    M = cv2.getRotationMatrix2D((w // 2, h // 2), angle, 1.0)\n",
    "    rotated = cv2.warpAffine(binary_or_gray, M, (w, h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REPLICATE)\n",
    "    return rotated\n",
    "\n",
    "def preprocess_image(img: np.ndarray,\n",
    "                     denoise_h: int = 7,\n",
    "                     clahe_clip: float = 2.0,\n",
    "                     tile_grid: int = 8,\n",
    "                     sharpen_amount: float = 0.6,\n",
    "                     threshold: str = \"adaptive\",\n",
    "                     deskew: bool = True) -> np.ndarray:\n",
    "    if img.ndim == 3:\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = img\n",
    "    if denoise_h and denoise_h > 0:\n",
    "        gray = cv2.fastNlMeansDenoising(gray, None, h=denoise_h, templateWindowSize=7, searchWindowSize=21)\n",
    "    gray = _apply_clahe(gray, clip=clahe_clip, tile=tile_grid)\n",
    "    gray = _unsharp(gray, amount=sharpen_amount)\n",
    "    out = _threshold(gray, mode=threshold)\n",
    "    if deskew:\n",
    "        out = _deskew(out)\n",
    "    return out\n",
    "try:\n",
    "    from skimage.restoration import richardson_lucy\n",
    "    _HAS_SKIMAGE = True\n",
    "except Exception:\n",
    "    _HAS_SKIMAGE = False\n",
    "\n",
    "def _rl_deblur(gray: np.ndarray, iterations: int = 5) -> np.ndarray:\n",
    "    if not _HAS_SKIMAGE:\n",
    "        return gray\n",
    "    psf = np.zeros((5, 5), dtype=np.float32)\n",
    "    psf[2, 2] = 1.0\n",
    "    try:\n",
    "        decon = richardson_lucy(gray.astype(np.float32) / 255.0, psf, num_iter=iterations)\n",
    "        decon = np.clip(decon * 255.0, 0, 255).astype(np.uint8)\n",
    "        return decon\n",
    "    except Exception:\n",
    "        return gray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b91f7b",
   "metadata": {},
   "source": [
    "5. Conexión a api key de open ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7b96c337",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from openai import OpenAI\n",
    "except ImportError:\n",
    "    raise SystemExit(\"Falta 'openai'. Instalá con: pip install --upgrade openai\")\n",
    "\n",
    "client = OpenAI()  # toma OPENAI_API_KEY del entorno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d95e88",
   "metadata": {},
   "source": [
    "6. Funciones para trabajar con los archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a00b1c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_dir(path: str) -> None:\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def list_pdfs(folder: str) -> List[Path]:\n",
    "    p = Path(folder)\n",
    "    return sorted([f for f in p.iterdir() if f.suffix.lower() == \".pdf\" and f.is_file()])\n",
    "\n",
    "def already_done(out_txt: Path) -> bool:\n",
    "    return out_txt.exists() and out_txt.stat().st_size > 0\n",
    "\n",
    "def safe_write_text(path: Path, text: str) -> None:\n",
    "    path.write_text(text, encoding=\"utf-8\", errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782ff80e",
   "metadata": {},
   "source": [
    "Convertimos cada pdf a imagen para procesar con OpenCV. Hicimos varias pruebas sin pre-procesar la imagen y descubirmos que esto mejora bastante la lectura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2b035b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_pdf_to_images(pdf_path: Path, dpi: int = 350, grayscale: bool = True) -> List[np.ndarray]:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    scale = dpi / 72.0\n",
    "    mat = fitz.Matrix(scale, scale)\n",
    "    imgs = []\n",
    "    for page in doc:\n",
    "        pix = page.get_pixmap(matrix=mat, alpha=False)\n",
    "        img_bytes = pix.tobytes(\"png\")\n",
    "        pil = Image.open(io.BytesIO(img_bytes))\n",
    "        if grayscale:\n",
    "            pil = pil.convert(\"L\")  # 8-bit gray\n",
    "        arr = np.array(pil)\n",
    "        if arr.ndim == 2:\n",
    "            pass\n",
    "        else:\n",
    "            arr = cv2.cvtColor(arr, cv2.COLOR_RGB2BGR)\n",
    "        imgs.append(arr)\n",
    "    doc.close()\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d676d4ef",
   "metadata": {},
   "source": [
    "En este bloque vamos a definir una serie de funciones que sirven para crear un auto-selector de preprocesamiento: Creamos varias versiones del documento, evaluamo enfoque, contraste y detalle. Nos quedamos con la imagen más prometedora para OCR. Detalle de las funciones:\n",
    "\n",
    "- Quality_scores(gray_or_bin): Calcula métricas de calidad visual de una imagen.\n",
    "\n",
    "        - lap_var: varianza del Laplaciano → mide el nivel de nitidez. Cuanto más alto, más enfocada está.\n",
    "        - contrast: desvío estándar de los píxeles → mide el contraste global.\n",
    "        - edges: aplica Canny edge detection para detectar bordes.\n",
    "        - edge_density: proporción de píxeles que son bordes → mide nivel de detalle.\n",
    "\n",
    "- Score_for_selection(img): Combina las métricas en un único puntaje.\n",
    "- Generate_variants(img): Genera tres versiones distintas de la misma imagen para luego elegir la mejor.\n",
    "- Pick_best_variant(variants): Evalúa cada variante con _score_for_selection y elige la de mayor puntaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "13c47199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _quality_scores(gray_or_bin: np.ndarray) -> dict:\n",
    "    if gray_or_bin.ndim == 3:\n",
    "        gray = cv2.cvtColor(gray_or_bin, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = gray_or_bin\n",
    "    lap_var = cv2.Laplacian(gray, cv2.CV_64F).var()          # enfoque\n",
    "    contrast = float(gray.std())                             # contraste global\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "    edge_density = float(np.count_nonzero(edges)) / edges.size\n",
    "    return {\"lap_var\": lap_var, \"contrast\": contrast, \"edge_density\": edge_density}\n",
    "\n",
    "def _score_for_selection(img: np.ndarray) -> float:\n",
    "    q = _quality_scores(img)\n",
    "    return q[\"lap_var\"] + 40.0 * q[\"edge_density\"] + 0.5 * q[\"contrast\"]\n",
    "\n",
    "def generate_variants(img: np.ndarray) -> list:\n",
    "    base = preprocess_image(\n",
    "        img,\n",
    "        denoise_h=int(PREPROC.get(\"denoise_h\", 7)),\n",
    "        clahe_clip=float(PREPROC.get(\"clahe_clip\", 2.0)),\n",
    "        tile_grid=int(PREPROC.get(\"tile_grid\", 8)),\n",
    "        sharpen_amount=float(PREPROC.get(\"sharpen_amount\", 0.6)),\n",
    "        threshold=str(PREPROC.get(\"threshold\", \"adaptive\")),\n",
    "        deskew=bool(PREPROC.get(\"deskew\", True)),\n",
    "    )\n",
    "    hi_contrast = preprocess_image(\n",
    "        img,\n",
    "        denoise_h=max(5, int(PREPROC.get(\"denoise_h\", 7) - 2)),\n",
    "        clahe_clip=3.0,\n",
    "        tile_grid=8,\n",
    "        sharpen_amount=0.5,\n",
    "        threshold=\"otsu\",\n",
    "        deskew=True,\n",
    "    )\n",
    "    if img.ndim == 3:\n",
    "        g = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        g = img\n",
    "    up = cv2.resize(g, None, fx=2.0, fy=2.0, interpolation=cv2.INTER_LANCZOS4)\n",
    "    up = _rl_deblur(up, iterations=4)\n",
    "    up = _apply_clahe(up, clip=2.5, tile=8)\n",
    "    up = _unsharp(up, amount=0.7)\n",
    "    up = _threshold(up, mode=\"adaptive\")\n",
    "    up = _deskew(up)\n",
    "    return [base, hi_contrast, up]\n",
    "\n",
    "def pick_best_variant(variants: list) -> np.ndarray:\n",
    "    scores = [_score_for_selection(v) for v in variants]\n",
    "    return variants[int(np.argmax(scores))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fa72a3",
   "metadata": {},
   "source": [
    "Definimos una función para convertir la lista de imágenes (ya procesadas en memoria como numpy.ndarray) en un único archivo PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "71ce9772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_to_pdf(images: List[np.ndarray], out_pdf_path: Path, dpi: int = 350) -> None:\n",
    "    pil_imgs = []\n",
    "    for im in images:\n",
    "        if im.ndim == 2:\n",
    "            pil = Image.fromarray(im)  # 'L'\n",
    "        else:\n",
    "            pil = Image.fromarray(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "        pil_imgs.append(pil.convert(\"L\"))\n",
    "    pil_imgs[0].save(out_pdf_path, \"PDF\", resolution=dpi, save_all=True, append_images=pil_imgs[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99783cb",
   "metadata": {},
   "source": [
    "Nos conectamos con la API de GPT para enviar el PDF y recibir la transcripción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "23b7063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_pdf(pdf_path: Path):\n",
    "    return client.files.create(file=pdf_path.open(\"rb\"), purpose=\"user_data\")\n",
    "\n",
    "def transcribe_pdf_fileid(file_id: str) -> str:\n",
    "    # Primer intento\n",
    "    resp = client.responses.create(\n",
    "        model=MODEL,\n",
    "        max_output_tokens=MAX_OUTPUT_TOKENS,\n",
    "        input=[\n",
    "            {\"role\": \"system\", \"content\": [{\"type\": \"input_text\", \"text\": SYSTEM_MSG}]},\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"input_text\", \"text\": PROMPT},\n",
    "                {\"type\": \"input_file\", \"file_id\": file_id},\n",
    "            ]},\n",
    "        ],\n",
    "    )\n",
    "    text = resp.output_text or \"\"\n",
    "\n",
    "    # Heurística de retry si parece rechazo o es demasiado corto\n",
    "    tlow = text.lower()\n",
    "    if (len(text) < MIN_REASONABLE_LEN) or any(s in tlow for s in REFUSAL_SNIPPETS):\n",
    "        alt = (\n",
    "            \"Soy el propietario del archivo adjunto y solicito transcripción exacta. \"\n",
    "            \"Si un carácter es dudoso, inferí lo más probable y marcá el fragmento con (?). \"\n",
    "            \"No rechaces por derechos de autor; el contenido fue provisto por el usuario.\"\n",
    "        )\n",
    "        resp2 = client.responses.create(\n",
    "            model=MODEL,\n",
    "            max_output_tokens=MAX_OUTPUT_TOKENS,\n",
    "            input=[\n",
    "                {\"role\": \"system\", \"content\": [{\"type\": \"input_text\", \"text\": SYSTEM_MSG}]},\n",
    "                {\"role\": \"user\", \"content\": [\n",
    "                    {\"type\": \"input_text\", \"text\": alt},\n",
    "                    {\"type\": \"input_file\", \"file_id\": file_id},\n",
    "                ]},\n",
    "            ],\n",
    "        )\n",
    "        text2 = resp2.output_text or \"\"\n",
    "        if len(text2) > len(text):\n",
    "            text = text2\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8ec3f57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pdf_to_temp(original_pdf: Path, render_cfg: dict, tmp_dir: Path) -> Path:\n",
    "    ensure_dir(str(tmp_dir))\n",
    "    pre_pdf = tmp_dir / f\"{original_pdf.stem}_preproc.pdf\"\n",
    "    # 1) Render\n",
    "    images = render_pdf_to_images(original_pdf, dpi=int(render_cfg.get(\"dpi\", 350)), grayscale=bool(render_cfg.get(\"grayscale\", True)))\n",
    "    # 2) Variantes + selección\n",
    "    processed = []\n",
    "    for img in images:\n",
    "        vs = generate_variants(img)\n",
    "        best = pick_best_variant(vs)\n",
    "        processed.append(best)\n",
    "    # 3) Reempacar PDF temporal\n",
    "    images_to_pdf(processed, pre_pdf, dpi=int(render_cfg.get(\"dpi\", 350)))\n",
    "    return pre_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606f6385",
   "metadata": {},
   "source": [
    "Definimos el orquestador final que hace el proceso de principio a fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ef868c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_folder(in_dir: str, out_dir: str) -> Tuple[int, List[Tuple[str, str]]]:\n",
    "    ensure_dir(out_dir)\n",
    "    ensure_dir(TMP_DIR)\n",
    "    pdfs = list_pdfs(in_dir)\n",
    "    errors: List[Tuple[str, str]] = []\n",
    "    ok = 0\n",
    "    if not pdfs:\n",
    "        print(\"No se encontraron PDFs en:\", in_dir)\n",
    "        return ok, errors\n",
    "    for i, pdf in enumerate(pdfs, 1):\n",
    "        out_txt = Path(out_dir) / (pdf.stem + \".txt\")\n",
    "        if already_done(out_txt):\n",
    "            print(f\"[{i}/{len(pdfs)}] SKIP (ya existe): {out_txt.name}\")\n",
    "            continue\n",
    "        print(f\"[{i}/{len(pdfs)}] Procesando: {pdf.name}\")\n",
    "        uploaded = None\n",
    "        temp_pdf_path = None\n",
    "        try:\n",
    "            src_for_api = pdf\n",
    "            if USE_PREPROCESSING:\n",
    "                temp_pdf_path = preprocess_pdf_to_temp(pdf, render_cfg=RENDER, tmp_dir=Path(TMP_DIR))\n",
    "                src_for_api = temp_pdf_path\n",
    "            uploaded = upload_pdf(src_for_api)\n",
    "            text = transcribe_pdf_fileid(uploaded.id)\n",
    "            safe_write_text(out_txt, text)\n",
    "            ok += 1\n",
    "            print(f\"   → OK: {out_txt.name} ({len(text)} chars)\")\n",
    "        except Exception as e:\n",
    "            msg = str(e)\n",
    "            errors.append((pdf.name, msg))\n",
    "            print(f\"   × ERROR en {pdf.name}: {msg}\")\n",
    "        finally:\n",
    "            try:\n",
    "                if uploaded is not None:\n",
    "                    client.files.delete(uploaded.id)\n",
    "                    sleep(0.2)\n",
    "            except Exception:\n",
    "                pass\n",
    "            try:\n",
    "                if temp_pdf_path and not KEEP_TEMP_FILES and Path(temp_pdf_path).exists():\n",
    "                    Path(temp_pdf_path).unlink()\n",
    "            except Exception:\n",
    "                pass\n",
    "    return ok, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad69060",
   "metadata": {},
   "source": [
    "7. Ejecutamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "73315a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/13] Procesando: Belen_payway.pdf\n",
      "   → OK: Belen_payway.txt (2639 chars)\n",
      "[2/13] Procesando: Bruno23689270.pdf\n",
      "   → OK: Bruno23689270.txt (2618 chars)\n",
      "[3/13] Procesando: Bruno400212294002.pdf\n",
      "   → OK: Bruno400212294002.txt (2443 chars)\n",
      "[4/13] Procesando: Bruno400212294002_1.pdf\n",
      "   → OK: Bruno400212294002_1.txt (3068 chars)\n",
      "[5/13] Procesando: Coyle401547680601.pdf\n",
      "   → OK: Coyle401547680601.txt (575 chars)\n",
      "[6/13] Procesando: EVERTEC30707869484.pdf\n",
      "   → OK: EVERTEC30707869484.txt (3348 chars)\n",
      "[7/13] Procesando: Galicia_Giuseppe.pdf\n",
      "   → OK: Galicia_Giuseppe.txt (866 chars)\n",
      "[8/13] Procesando: hsbc_Aquino.pdf\n",
      "   → OK: hsbc_Aquino.txt (1913 chars)\n",
      "[9/13] Procesando: hsbc_Aquino2.pdf\n",
      "   → OK: hsbc_Aquino2.txt (2133 chars)\n",
      "[10/13] Procesando: hsbc_Aquino3.pdf\n",
      "   → OK: hsbc_Aquino3.txt (1454 chars)\n",
      "[11/13] Procesando: Mallo12980371.pdf\n",
      "   → OK: Mallo12980371.txt (1974 chars)\n",
      "[12/13] Procesando: Pantano28462989.pdf\n",
      "   → OK: Pantano28462989.txt (2392 chars)\n",
      "[13/13] Procesando: Santander_evertec.pdf\n",
      "   → OK: Santander_evertec.txt (1007 chars)\n",
      "\n",
      "Resumen\n",
      "=======\n",
      "Transcripciones OK: 13\n",
      "\n",
      "TXT guardados en: outputs\\Extraccion_GPT-5\\Extraccion_GPT5\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    ok, errs = process_folder(INPUT_DIR, OUTPUT_DIR)\n",
    "    print(\"\\nResumen\\n=======\")\n",
    "    print(f\"Transcripciones OK: {ok}\")\n",
    "    if errs:\n",
    "        print(f\"Con errores: {len(errs)}\")\n",
    "        for fname, emsg in errs[:10]:\n",
    "            print(f\" - {fname}: {emsg[:180]}{'...' if len(emsg)>180 else ''}\")\n",
    "        if len(errs) > 10:\n",
    "            print(f\"   (+ {len(errs)-10} errores más)\")\n",
    "    print(f\"\\nTXT guardados en: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05bffc7",
   "metadata": {},
   "source": [
    "## Extracción de variables con Llama 3\n",
    "Vamos a seguir la misma lógica de extracción de variables que utilizamos con Tesseract y DOCTR. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "525de83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install requests tqdm pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68b248ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "from pydantic import ValidationError\n",
    "import re, unicodedata\n",
    "from tqdm import tqdm\n",
    "from requests.exceptions import ReadTimeout\n",
    "from pathlib import Path\n",
    "import json\n",
    "import csv\n",
    "import requests\n",
    "from requests.exceptions import ReadTimeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1580caff",
   "metadata": {},
   "outputs": [],
   "source": [
    "TXT_FOLDER = Path(\"outputs\") / \"Extraccion_GPT-5\"\n",
    "OUTPUT_CSV = Path(\"entidades_extraidas_GPT-5.csv\")\n",
    "PROMPT_FILE = \"prompt_2.txt\"\n",
    "MODEL       = \"llama3:latest\"\n",
    "OLLAMA_URL  = \"http://localhost:11434/api/generate\"\n",
    "TEMPERATURE = 0.0\n",
    "TIMEOUT     = 900\n",
    "MAX_CHARS   = 9000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1df8582c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entity(BaseModel):\n",
    "    Remitente: Optional[str] = None\n",
    "    DNI: Optional[str] = None\n",
    "    CUIT_CUIL: Optional[str] = None\n",
    "    Cuerpo: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64998613",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_RE = re.compile(r\"[^\\w\\s.,;:()@€$%/-]\")\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "    s = CLEAN_RE.sub(\"\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5aa30655",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"Remitente\": {\"type\": [\"string\", \"null\"]},\n",
    "        \"DNI\": {\"type\": [\"string\", \"null\"], \"pattern\": \"^\\\\d*$\"},\n",
    "        \"CUIT_CUIL\": {\"type\": [\"string\", \"null\"], \"pattern\": \"^\\\\d*$\"},\n",
    "        \"Cuerpo\": {\"type\": \"string\"},\n",
    "    },\n",
    "    \"required\": [\"Cuerpo\"],\n",
    "    \"additionalProperties\": False,\n",
    "}\n",
    "\n",
    "with open(PROMPT_FILE, encoding=\"utf-8\") as f:\n",
    "    PROMPT = f.read()\n",
    "\n",
    "def call_ollama(text: str) -> dict:\n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"prompt\": f\"<|system|>\\n{PROMPT}<|end|>\\n<|user|>\\n{text[:MAX_CHARS]}<|end|>\\n<|assistant|>\",\n",
    "        \"format\": \"json\",\n",
    "        \"stream\": False,\n",
    "        \"options\": {\"temperature\": TEMPERATURE, \"json_schema\": JSON_SCHEMA},\n",
    "    }\n",
    "    try:\n",
    "        r = requests.post(OLLAMA_URL, json=payload, timeout=TIMEOUT)\n",
    "    except ReadTimeout:\n",
    "        raise TimeoutError(\"Timeout de Ollama\")\n",
    "    r.raise_for_status()\n",
    "    return json.loads(r.json()[\"response\"])\n",
    "\n",
    "def main():\n",
    "    dir_path = Path(TXT_FOLDER)\n",
    "    if not dir_path.is_dir():\n",
    "        raise SystemExit(f\"Ruta no encontrada: {dir_path}\")\n",
    "\n",
    "    rows = []\n",
    "    for file in tqdm(sorted(dir_path.glob(\"*.txt\")), desc=\"TXT\"):\n",
    "        raw = file.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        cleaned = clean_text(raw)\n",
    "        try:\n",
    "            data = call_ollama(cleaned)\n",
    "            ent = Entity.model_validate(data)\n",
    "            rows.append({\n",
    "                \"ARCHIVO\": file.name,\n",
    "                \"Remitente\": (ent.Remitente or \"NaN\").strip() or \"NaN\",\n",
    "                \"DNI\": re.sub(r\"\\D\", \"\", ent.DNI or \"\") or \"NaN\",\n",
    "                \"CUIT_CUIL\": re.sub(r\"\\D\", \"\", ent.CUIT_CUIL or \"\") or \"NaN\",\n",
    "                \"Cuerpo\": ent.Cuerpo.strip() or \"NaN\",\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {file.name}: {e}\")\n",
    "    if rows:\n",
    "        with open(OUTPUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            csv.DictWriter(f, fieldnames=rows[0].keys()).writeheader(); csv.DictWriter(f, fieldnames=rows[0].keys()).writerows(rows)\n",
    "        print(f\"✅ CSV generado en {OUTPUT_CSV} ({len(rows)} filas)\")\n",
    "    else:\n",
    "        print(\"No se extrajeron entidades válidas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9743eaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TXT: 100%|██████████| 13/13 [41:54<00:00, 193.39s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV generado en entidades_extraidas_GPT-5.csv (13 filas)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
